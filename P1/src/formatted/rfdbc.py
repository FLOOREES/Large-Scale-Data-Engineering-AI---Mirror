import itertools
from typing import Dict, Any, List
import json

# --- PySpark Imports ---
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import (
    StructType, StructField, StringType, DoubleType
)
# ---------------------

class RFDBCFormattedZone:
    """
    Transforms nested JSON-like data (RFDBC structure) into a flat structure
    with custom output column names and writes it to a Delta Lake table
    in the Formatted Zone using PySpark.
    """

    def __init__(self, spark: SparkSession, output_path: str = "./data/formatted/rfdbc", input_data_path: str = "./data/landing/rfdbc.json"):
        self.spark = spark
        self.input_data_path = input_data_path
        self.output_path = output_path
        # Define the mapping from original generated names to desired final names
        self.column_mapping = self._get_column_mapping()
        # Define the schema matching the *output* of _transform_data
        self.intermediate_schema = self._define_intermediate_schema()
        # Define the *final* desired schema with the new names
        self.target_schema = self._define_target_schema()
        print(f"RFDBC Formatted Zone Initialized. Output Delta Path: {self.output_path}")
        print(f"Column Renaming Map: {self.column_mapping}")

    def _load_input_data(self) -> Dict[str, Any]:
        """Loads the input data from a JSON file."""
        try:
            with open(self.input_data_path, 'r') as file:
                data = json.load(file)
            print(f"Input data loaded successfully from {self.input_data_path}.")
            return data
        except Exception as e:
            print(f"Error loading input data from {self.input_data_path}: {e}")
            return {}

    def _get_column_mapping(self) -> Dict[str, str]:
        """
        Defines the mapping from the column names generated by _transform_data
        to the desired final column names for the Delta table.
        Keys: Original names (e.g., 'YEAR_CODE').
        Values: New desired names (e.g., 'ANIO_CODIGO').
        """
        return {
            "YEAR_CODE": "year_code",
            "YEAR_LABEL": "year_label",
            "MUN_CODE": "municipality_id",
            "MUN_LABEL": "municipality_name",
            "CONCEPT_CODE": "concept_code",
            "CONCEPT_LABEL": "concept_label",
            "INDICATOR_CODE": "indicator_code",
            "INDICATOR_LABEL": "indicator_label",
            "VALUE": "value"
        }

    def _define_intermediate_schema(self) -> StructType:
        """
        Defines the schema that matches the structure produced *directly*
        by the _transform_data function (using original dimension names).
        """
        return StructType([
            StructField("YEAR_CODE", StringType(), True),
            StructField("YEAR_LABEL", StringType(), True),
            StructField("MUN_CODE", StringType(), True),
            StructField("MUN_LABEL", StringType(), True),
            StructField("CONCEPT_CODE", StringType(), True),
            StructField("CONCEPT_LABEL", StringType(), True),
            StructField("INDICATOR_CODE", StringType(), True),
            StructField("INDICATOR_LABEL", StringType(), True),
            StructField("VALUE", DoubleType(), True)
        ])

    def _define_target_schema(self) -> StructType:
        """
        Defines the FINAL desired schema for the formatted Delta table
        using the NEW column names.
        """
        # Uses the values from the column_mapping
        mapping = self.column_mapping
        return StructType([
            StructField(mapping.get("YEAR_CODE", "YEAR_CODE"), StringType(), True),
            StructField(mapping.get("YEAR_LABEL", "YEAR_LABEL"), StringType(), True),
            StructField(mapping.get("MUN_CODE", "MUN_CODE"), StringType(), True),
            StructField(mapping.get("MUN_LABEL", "MUN_LABEL"), StringType(), True),
            StructField(mapping.get("CONCEPT_CODE", "CONCEPT_CODE"), StringType(), True),
            StructField(mapping.get("CONCEPT_LABEL", "CONCEPT_LABEL"), StringType(), True),
            StructField(mapping.get("INDICATOR_CODE", "INDICATOR_CODE"), StringType(), True),
            StructField(mapping.get("INDICATOR_LABEL", "INDICATOR_LABEL"), StringType(), True),
            StructField(mapping.get("VALUE", "VALUE"), DoubleType(), True)
        ])

    def _transform_data(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Transforms the nested JSON-like dictionary data into a list of flat records (rows).
        This function remains UNCHANGED and produces keys like 'YEAR_CODE', 'MUN_CODE', etc.
        """
        if not data or 'dimension' not in data or 'value' not in data or 'id' not in data:
            print("Error: Input data structure is invalid or missing key components.")
            return []

        try:
            dimensions = data['id']
            if 'dimension' not in data:
                print("Error: Missing 'dimension' key in input data.")
                return []

            dim_categories = {}
            dim_labels = {}
            for dim in dimensions:
                # Defensive checks for nested structure
                if dim not in data['dimension']:
                    print(f"Error: Dimension '{dim}' listed in 'id' not found in 'dimension' object.")
                    return []
                if 'category' not in data['dimension'][dim]:
                     print(f"Error: Missing 'category' for dimension '{dim}'.")
                     return []
                if 'index' not in data['dimension'][dim]['category']:
                     print(f"Error: Missing 'category.index' for dimension '{dim}'.")
                     return []

                dim_categories[dim] = data['dimension'][dim]['category']['index']
                dim_labels[dim] = data['dimension'][dim]['category'].get('label', {}) # Safely get labels


            values = data.get('value', [])

            category_lists = [dim_categories[dim] for dim in dimensions]
            combinations = itertools.product(*category_lists)

            records = []
            value_iter = iter(values)

            # --- Calculate expected size for validation ---
            expected_size = 1
            try:
                for dim in dimensions:
                    # Check if dim exists and has categories before calculation
                    if dim in dim_categories and dim_categories[dim] is not None:
                         expected_size *= len(dim_categories[dim])
                    else:
                         print(f"Warning: Dimension '{dim}' has no categories for size calculation.")
                         expected_size = 0 # Indicate size cannot be calculated reliably
                         break
            except Exception as calc_e:
                print(f"Warning: Could not calculate expected size due to issue with dim_categories: {calc_e}")
                expected_size = -1 # Indicate calculation failed

            # --- Process Combinations ---
            processed_count = 0
            for combo in combinations:
                try:
                    raw_value = next(value_iter)
                except StopIteration:
                    print(f"Warning: Ran out of values after processing {processed_count} records. Expected based on dimensions: {expected_size if expected_size >= 0 else 'unknown'}.")
                    break # Stop processing if values run out

                record = {}
                combo_dict = dict(zip(dimensions, combo))

                # Dynamically create CODE and LABEL fields for each dimension
                for dim in dimensions:
                    code = combo_dict[dim]
                    record[f"{dim}_CODE"] = str(code) # Ensure code is string
                    label = dim_labels[dim].get(code, str(code)) # Default label to code if missing
                    record[f"{dim}_LABEL"] = str(label) # Ensure label is string

                # Process the value
                if raw_value is not None:
                    try:
                        # Attempt conversion, handle potential errors gracefully
                        record['VALUE'] = float(raw_value)
                    except (ValueError, TypeError):
                        # print(f"Warning: Could not convert value '{raw_value}' to float for combination {combo}. Setting VALUE to None.")
                        record['VALUE'] = None # Set to None if conversion fails
                else:
                    record['VALUE'] = None # Keep None as None

                records.append(record)
                processed_count += 1

            # --- Post-processing Validation ---
            if expected_size >= 0 and len(values) != expected_size:
                print(f"Warning: Number of values in input data ({len(values)}) does not match calculated expected size based on dimensions ({expected_size}). Data might be incomplete or dimension definitions inconsistent.")

            if processed_count != len(values) and not (expected_size >= 0 and processed_count == expected_size):
                 print(f"Warning: Number of generated records ({processed_count}) differs from number of values ({len(values)}) and potentially the expected size ({expected_size if expected_size >=0 else 'unknown'}). Check for data inconsistencies or value list truncation.")


            return records

        except KeyError as e:
            print(f"Error: Missing expected key in data during transformation: {e}")
            import traceback
            traceback.print_exc()
            return []
        except Exception as e:
            print(f"An unexpected error occurred during data transformation: {e}")
            import traceback
            traceback.print_exc()
            return []

    def run(self):
        """Executes the Formatted Zone processing for the RFDBC data."""
        print(f"--- Running RFDBC Formatted Zone Task ---")
        self.input_data = self._load_input_data()
        
        try:
            # 1. Transform the input Python dictionary data
            print("Transforming raw data dictionary into records...")
            records = self._transform_data(self.input_data)

            if not records:
                print("No records generated from data transformation. Skipping Delta write.")
                print("--- RFDBC Formatted Zone Task Completed (No Data Written) ---")
                return

            print(f"Successfully transformed data into {len(records)} records.")

            # 2. Create Spark DataFrame using the INTERMEDIATE schema
            print("Creating intermediate Spark DataFrame...")
            df_intermediate = self.spark.createDataFrame(records, schema=self.intermediate_schema)

            print("Intermediate Spark DataFrame created successfully. Schema:")
            df_intermediate.printSchema()

            # 3. Rename columns to match the TARGET schema
            print("Renaming columns for final output...")
            df_formatted = df_intermediate
            select_exprs = []
            for original_name, new_name in self.column_mapping.items():
                 if original_name in df_intermediate.columns:
                     select_exprs.append(F.col(original_name).alias(new_name))
                 else:
                     print(f"Warning: Column '{original_name}' defined in mapping not found in intermediate DataFrame. Skipping rename for this column.")

            # Ensure all columns intended for the target schema are selected
            if len(select_exprs) != len(self.target_schema.fields):
                 print(f"Warning: Number of selected columns ({len(select_exprs)}) after renaming does not match target schema fields ({len(self.target_schema.fields)}). Check mapping and intermediate schema.")
                 final_columns = [expr.alias for expr in select_exprs]
                 print(f"Proceeding with columns: {final_columns}")
                 df_formatted = df_intermediate.select(*select_exprs)

            else:
                 df_formatted = df_intermediate.select(*select_exprs)


            print("Columns renamed. Final Schema:")
            df_formatted.printSchema()
            if df_formatted.schema != self.target_schema:
                print("Warning: Renamed DataFrame schema does not exactly match target schema definition. This might happen due to ordering or slight type differences if not strictly controlled.")
                print("Actual Schema:", df_formatted.schema)
                print("Target Schema:", self.target_schema)


            print("Sample data with new column names (first 5 rows):")
            df_formatted.show(5, truncate=False)

            # 4. Write to Delta Lake using the final DataFrame
            print(f"Writing formatted data to Delta table: {self.output_path}")
            df_formatted.write.format("delta") \
                .mode("overwrite") \
                .option("overwriteSchema", "true") \
                .save(self.output_path)

            print(f"Formatted data successfully written to {self.output_path}")
            print("--- RFDBC Formatted Zone Task Successfully Completed ---")

        except Exception as e:
            print(f"!!! Error during RFDBC Formatted Zone execution: {e}")
            import traceback
            traceback.print_exc()
            print("--- RFDBC Formatted Zone Task Failed ---")

# --- Main Execution Block ---
if __name__ == "__main__":
    from spark_session import get_spark_session
    spark = None
    try:
        spark = get_spark_session()

        formatter = RFDBCFormattedZone(
            spark=spark
        )
        formatter.run()

    except Exception as main_error:
        print(f"An error occurred in the main execution block: {main_error}")
        import traceback
        traceback.print_exc()
    finally:
        if spark:
            print("Stopping Spark Session.")
            spark.stop()