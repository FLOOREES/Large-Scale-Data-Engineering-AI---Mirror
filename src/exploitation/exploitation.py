# src/exploitation/exploitation.py

import json
from pathlib import Path
import traceback
from tqdm import tqdm
import pandas as pd

# --- RDFLib Imports ---
from rdflib import Graph, Literal, Namespace
from rdflib.namespace import RDF, RDFS, XSD

# --- PySpark Imports ---
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql import functions as F
from pyspark.sql.window import Window
from pyspark.sql.types import DoubleType

import geo_relations

# --- Configuration ---
PROJ = Namespace("http://example.com/catalonia-ontology/")
TRUSTED_DIR = Path("./data/trusted/")
RELATIONS_DIR = Path("./data/relations/")
EXPLOITATION_DIR = Path("./data/exploitation/")
EXPLOITATION_DIR.mkdir(parents=True, exist_ok=True)

class ExploitationZone:
    """
    Orchestrates the creation of two key assets in the Exploitation Zone:
    1. A feature-rich, consolidated annual tabular dataset for ML tasks.
    2. A comprehensive Knowledge Graph for semantic queries and embeddings.
    """

    def __init__(self, spark: SparkSession):
        self.spark = spark
        geo_relations.main() # Ensure geo-relations are up-to-date

        self.graph = self._initialize_graph()
        self.all_desired_indicators = {
            'f171': 'population', 'f36': 'population_men', 'f42': 'population_women',
            'f187': 'births_total', 'f183': 'population_spanish_nat',
            'f261': 'surface_km2', 'f262': 'density_pop_km2', 'f328': 'longitude',
            'f329': 'latitude', 'f308': 'unemployment_total', 'f191': 'total_family_dwellings',
            'f270': 'public_libraries_count', 'f293': 'sports_pavilions_count',
            'f294': 'multi_sports_courts_count', 'f301': 'indoor_pools_count'
        }
        self.indicator_map = self.all_desired_indicators # Alias for KG population
        print("Exploitation Zone Initialized.")

    def _initialize_graph(self) -> Graph:
        g = Graph()
        g.bind("proj", PROJ)
        g.bind("rdf", RDF)
        g.bind("rdfs", RDFS)
        g.bind("xsd", XSD)
        return g

    def _load_data(self) -> dict:
        print("\n--- Step 1: Loading all trusted data sources ---")
        data = {}
        try:
            data['idescat'] = self.spark.read.format("delta").load(str(TRUSTED_DIR / "idescat"))
            data['lloguer'] = self.spark.read.format("delta").load(str(TRUSTED_DIR / "lloguer"))
            data['rfdbc'] = self.spark.read.format("delta").load(str(TRUSTED_DIR / "rfdbc"))
            
            with open(RELATIONS_DIR / "municipality_neighbors.json", 'r') as f: data['mun_neighbors'] = json.load(f)
            with open(RELATIONS_DIR / "comarca_neighbors.json", 'r') as f: data['com_neighbors'] = json.load(f)
            with open(RELATIONS_DIR / "comarca_to_province.json", 'r') as f: data['com_to_prov'] = json.load(f)
            print("All trusted data and relation files loaded successfully.")
            return data
        except Exception as e:
            print(f"ERROR: Could not load one or more data sources. Details: {e}")
            raise

    def _create_consolidated_table(self, data: dict) -> DataFrame:
        """
        Creates the feature-rich consolidated table using the robust logic from the old project.
        """
        print("\n--- Step 2: Creating Consolidated Annual Table for ML ---")

        # --- Process Lloguer (Rent) Data ---
        print("Processing Lloguer data...")
        df_lloguer_imputed = data['lloguer'].filter(F.col("ambit_territorial") == "Municipi").withColumn(
            "renda_imputed", F.when(F.col("renda").isNotNull(), F.col("renda"))
             .when(F.col("tram_preus") == "<= 350 euros/mes", 325.0).when(F.col("tram_preus") == "> 350 i <= 450 euros/mes", 400.0)
             .when(F.col("tram_preus") == "> 350 i <= 500 euros/mes", 425.0).when(F.col("tram_preus") == "> 450 i <= 600 euros/mes", 525.0)
             .when(F.col("tram_preus") == "> 500 i <= 650 euros/mes", 575.0).when(F.col("tram_preus") == "> 600 euros/mes", 800.0)
             .when(F.col("tram_preus") == "> 650 euros/mes", 900.0).otherwise(None))
        
        df_lloguer_annual = df_lloguer_imputed.groupBy("codi_territorial", "any").agg(
            F.first("nom_territori").alias("municipality_name"),
            F.sum("habitatges").alias("total_contracts"),
            (F.sum(F.col("renda_imputed") * F.col("habitatges")) / F.sum("habitatges")).alias("avg_monthly_rent")
        ).withColumnRenamed("codi_territorial", "municipality_id")

        # --- Process RFDBC (Salary) Data ---
        print("Processing RFDBC data...")
        df_rfdbc_processed = data['rfdbc'].filter(F.col("indicator_code") == "PER_CAPITA_EUR").select(
            F.col("municipality_id"), F.col("year").cast("int").alias("any"), F.col("value").alias("income_per_capita"))

        # --- Join Annual Data (Rent and Salary) ---
        print("Joining annual rent and salary data...")
        df_merged_annual = df_lloguer_annual.join(df_rfdbc_processed, ["municipality_id", "any"], "full_outer")

        # --- Process Idescat: Get LATEST available value for each indicator ---
        print("Processing Idescat data to get latest static values...")
        window_spec = Window.partitionBy("municipality_id", "indicator_id").orderBy(F.col("reference_year").desc())
        df_idescat_latest = data['idescat'].withColumn("rank", F.row_number().over(window_spec)).filter(F.col("rank") == 1).drop("rank")
        
        df_idescat_pivoted = df_idescat_latest.groupBy("municipality_id", "comarca_name").pivot("indicator_id", list(self.all_desired_indicators.keys())).agg(F.first("municipality_value"))
        
        for old_name, new_name in self.all_desired_indicators.items():
            if old_name in df_idescat_pivoted.columns:
                df_idescat_pivoted = df_idescat_pivoted.withColumnRenamed(old_name, new_name)
        
        # --- Final Join: Join annual data with static (latest) Idescat data ---
        print("Joining annual data with static Idescat indicators...")
        df_final = df_merged_annual.join(
            F.broadcast(df_idescat_pivoted), on="municipality_id", how="left"
        )
        
        print("Consolidated table for ML created successfully.")
        return df_final

    def _populate_graph(self, idescat_df: DataFrame, annual_df: DataFrame, relations: dict):
        print("\n--- Step 3: Populating the Knowledge Graph ---")
        mun_neighbors, com_neighbors, com_to_prov = relations['mun_neighbors'], relations['com_neighbors'], relations['com_to_prov']
        created_nodes = set()
        
        print("Generating structural nodes and relationships...")
        geo_structure_df = idescat_df.select("municipality_id", "municipality_name", "comarca_name").distinct()
        for row in tqdm(geo_structure_df.collect(), desc="Creating Geo Entities"):
            mun_id, mun_name, com_name = row['municipality_id'], row['municipality_name'], row['comarca_name']
            if not mun_id or not com_name: continue

            mun_uri = PROJ[f"municipality/{mun_id}"]
            com_uri = PROJ[f"comarca/{com_name.replace(' ', '_')}"]
            
            if mun_uri not in created_nodes:
                self.graph.add((mun_uri, RDF.type, PROJ.Municipality))
                self.graph.add((mun_uri, RDFS.label, Literal(mun_name, lang='ca')))
                self.graph.add((mun_uri, PROJ.isInComarca, com_uri))
                created_nodes.add(mun_uri)

            if com_uri not in created_nodes:
                prov_name = com_to_prov.get(com_name)
                self.graph.add((com_uri, RDF.type, PROJ.Comarca))
                self.graph.add((com_uri, RDFS.label, Literal(com_name, lang='ca')))
                if prov_name:
                    prov_uri = PROJ[f"province/{prov_name.replace(' ', '_')}"]
                    self.graph.add((com_uri, PROJ.isInProvince, prov_uri))
                    if prov_uri not in created_nodes:
                        self.graph.add((prov_uri, RDF.type, PROJ.Province))
                        self.graph.add((prov_uri, RDFS.label, Literal(prov_name, lang='ca')))
                        created_nodes.add(prov_uri)
                created_nodes.add(com_uri)

        for mun_id, data in mun_neighbors.items():
            for neighbor in data['neighbors']:
                self.graph.add((PROJ[f"municipality/{mun_id}"], PROJ.isNeighborOf, PROJ[f"municipality/{neighbor['id']}"]))
        for com_name, data in com_neighbors.items():
            for neighbor_name in data['neighbors']:
                self.graph.add((PROJ[f"comarca/{com_name.replace(' ', '_')}"], PROJ.isAdjacentTo, PROJ[f"comarca/{neighbor_name.replace(' ', '_')}"]))

        print("Generating nodes for Idescat indicator concepts...")
        for indicator_name in self.indicator_map.values():
            indicator_uri = PROJ[f"indicator/{indicator_name}"]
            if indicator_uri not in created_nodes:
                self.graph.add((indicator_uri, RDF.type, PROJ.IdescatIndicator))
                self.graph.add((indicator_uri, RDFS.label, Literal(indicator_name)))
                created_nodes.add(indicator_uri)

        print("Generating Idescat latest-value observations...")
        window_spec = Window.partitionBy("municipality_id", "indicator_id").orderBy(F.col("reference_year").desc())
        idescat_latest_df = idescat_df.withColumn("rank", F.row_number().over(window_spec)).filter(F.col("rank") == 1).drop("rank")
        
        for row in tqdm(idescat_latest_df.collect(), desc="Creating Idescat Observations"):
            mun_id, indicator_id, value, ref_year = row['municipality_id'], row['indicator_id'], row['municipality_value'], row['reference_year']
            if any(v is None for v in [mun_id, indicator_id, value, ref_year]): continue

            indicator_name = self.indicator_map.get(indicator_id)
            if not indicator_name: continue

            obs_uri = PROJ[f"observation/{mun_id}_{indicator_name}"]
            mun_uri = PROJ[f"municipality/{mun_id}"]
            indicator_uri = PROJ[f"indicator/{indicator_name}"]

            self.graph.add((obs_uri, RDF.type, PROJ.IndicatorObservation))
            self.graph.add((obs_uri, RDFS.label, Literal(f"{mun_id}_{indicator_name}")))
            self.graph.add((obs_uri, PROJ.value, Literal(value, datatype=XSD.double)))
            self.graph.add((obs_uri, PROJ.referenceYear, Literal(ref_year, datatype=XSD.gYear)))
            self.graph.add((mun_uri, PROJ.hasObservation, obs_uri))
            self.graph.add((obs_uri, PROJ.field, indicator_uri))

        print("Generating AnnualDataPoint nodes with all annual data...")
        for row in tqdm(annual_df.toLocalIterator(), desc="Creating Annual Data Points"):
            mun_id = row['municipality_id']
            year = row['any']
            
            if not mun_id or pd.isna(year): continue

            mun_uri = PROJ[f"municipality/{mun_id}"]
            datapoint_uri = PROJ[f"datapoint/{mun_id}_{int(year)}"]
            
            self.graph.add((datapoint_uri, RDF.type, PROJ.AnnualDataPoint))
            self.graph.add((mun_uri, PROJ.hasAnnualData, datapoint_uri))
            self.graph.add((datapoint_uri, PROJ.referenceYear, Literal(int(year), datatype=XSD.gYear)))
            
            if 'avg_monthly_rent' in row and pd.notna(row['avg_monthly_rent']):
                self.graph.add((datapoint_uri, PROJ.avgMonthlyRent, Literal(row['avg_monthly_rent'], datatype=XSD.double)))
            if 'total_contracts' in row and pd.notna(row['total_contracts']):
                self.graph.add((datapoint_uri, PROJ.totalContracts, Literal(row['total_contracts'], datatype=XSD.integer)))
            if 'income_per_capita' in row and pd.notna(row['income_per_capita']):
                self.graph.add((datapoint_uri, PROJ.incomePerCapita, Literal(row['income_per_capita'], datatype=XSD.double)))
        
        print("Knowledge Graph population complete.")


    def run(self):
        """Executes the full Exploitation Zone pipeline."""
        try:
            all_data = self._load_data()
            df_for_ml = self._create_consolidated_table(all_data)
            
            table_output_path = EXPLOITATION_DIR / "municipal_annual"
            print(f"\n--- Saving consolidated annual table to: {table_output_path} ---")
            df_for_ml.write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(str(table_output_path))
            print("Consolidated table for ML saved successfully.")
            
            relations_data = {k: v for k, v in all_data.items() if k.endswith('_neighbors') or k.endswith('_prov')}
            self. _populate_graph(all_data['idescat'], df_for_ml, relations_data)
            
            kg_output_file = EXPLOITATION_DIR / "knowledge_graph.ttl"
            print(f"\n--- Saving Knowledge Graph to {kg_output_file} ---")
            self.graph.serialize(destination=str(kg_output_file), format='turtle')
            print(f"Knowledge Graph saved successfully. Contains {len(self.graph)} triples.")
            
            print("\n--- Exploitation Zone (KG & ML Table) Task Successfully Completed ---")
        except Exception as e:
            print(f"!!! ERROR during Exploitation Zone execution: {e}")
            traceback.print_exc()


if __name__ == "__main__":
    from spark_session import get_spark_session
    spark = None
    try:
        spark = get_spark_session()
        processor = ExploitationZone(spark=spark)
        processor.run()
    except Exception as main_error:
        print(f"\nAn unexpected error occurred in the main execution block: {main_error}")
        traceback.print_exc()
    finally:
        if spark:
            print("\nStopping Spark Session.")
            spark.stop()
            print("Spark Session stopped.")