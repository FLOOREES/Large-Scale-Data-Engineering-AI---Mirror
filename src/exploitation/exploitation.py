# exploitation_zone_kg.py

import json
from pathlib import Path
import traceback
from tqdm import tqdm
import pandas as pd

# --- RDFLib Imports ---
from rdflib import Graph, URIRef, Literal, Namespace
from rdflib.namespace import RDF, RDFS, XSD

# --- PySpark Imports ---
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# --- Configuration ---
PROJ = Namespace("http://example.com/catalonia-ontology/")
TRUSTED_DIR = Path("./data/trusted/")
RELATIONS_DIR = Path("./data/relations/")
EXPLOITATION_DIR = Path("./data/exploitation/")
EXPLOITATION_DIR.mkdir(parents=True, exist_ok=True)

class ExploitationZone:
	"""
	Orchestrates the creation of a fully reified and comprehensive Knowledge Graph.
	This version ensures all trusted data is represented, including imputed rent,
	and all RFDBC indicators, correctly modeling them within the AnnualDataPoint nodes.
	"""

	def __init__(self, spark: SparkSession):
		self.spark = spark
		self.graph = self._initialize_graph()
		self.indicator_map = self._define_indicator_map()
		print("Knowledge Graph Exploitation Zone Initialized.")

	def _initialize_graph(self) -> Graph:
		"""Initializes an RDFLib Graph and binds common prefixes."""
		g = Graph()
		g.bind("proj", PROJ)
		g.bind("rdf", RDF)
		g.bind("rdfs", RDFS)
		g.bind("xsd", XSD)
		return g

	def _define_indicator_map(self) -> dict:
		"""
		Maps Idescat indicator IDs to clean, readable, lowerCamelCase names for the KG.
		"""
		return {
			'f171': 'population', 'f36': 'populationMen', 'f42': 'populationWomen',
			'f187': 'birthsTotal', 'f183': 'populationSpanishNat',
			'f261': 'surfaceKM2', 'f262': 'densityPopKM2',
			'f328': 'longitude', 'f329': 'latitude',
			'f308': 'unemploymentTotal', 'f191': 'totalFamilyDwellings',
			'f270': 'publicLibrariesCount', 'f293': 'sportsPavilionsCount',
			'f294': 'multiSportsCourtsCount', 'f301': 'indoorPoolsCount'
		}

	def _load_data(self) -> dict:
		"""Loads all necessary data sources (Delta tables and JSON files)."""
		print("\n--- Step 1: Loading all data sources ---")
		data = {}
		try:
			data['idescat'] = self.spark.read.format("delta").load(str(TRUSTED_DIR / "idescat"))
			data['lloguer'] = self.spark.read.format("delta").load(str(TRUSTED_DIR / "lloguer"))
			data['rfdbc'] = self.spark.read.format("delta").load(str(TRUSTED_DIR / "rfdbc"))
			print("Trusted Zone Delta tables loaded successfully.")

			with open(RELATIONS_DIR / "municipality_neighbors.json", 'r') as f: data['mun_neighbors'] = json.load(f)
			with open(RELATIONS_DIR / "comarca_neighbors.json", 'r') as f: data['com_neighbors'] = json.load(f)
			with open(RELATIONS_DIR / "comarca_to_province.json", 'r') as f: data['com_to_prov'] = json.load(f)
			print("Geographic relation JSON files loaded successfully.")
			return data
		except Exception as e:
			print(f"ERROR: Could not load one or more data sources. Details: {e}")
			raise

	def _process_data_for_kg(self, data: dict) -> dict:
		"""Processes and transforms Spark DataFrames to prepare for triple generation."""
		print("\n--- Step 2: Processing Spark DataFrames ---")
		
		print("Processing Lloguer data...")
		df_lloguer_imputed = data['lloguer'].filter(F.col("ambit_territorial") == "Municipi").withColumn(
			"renda_imputed",
			F.when(F.col("renda").isNotNull(), F.col("renda"))
			 .when(F.col("tram_preus") == "<= 350 euros/mes", 325.0).when(F.col("tram_preus") == "> 350 i <= 450 euros/mes", 400.0)
			 .when(F.col("tram_preus") == "> 350 i <= 500 euros/mes", 425.0).when(F.col("tram_preus") == "> 450 i <= 600 euros/mes", 525.0)
			 .when(F.col("tram_preus") == "> 500 i <= 650 euros/mes", 575.0).when(F.col("tram_preus") == "> 600 euros/mes", 800.0)
			 .when(F.col("tram_preus") == "> 650 euros/mes", 900.0).otherwise(None)
		)
		df_lloguer_annual = df_lloguer_imputed.groupBy("codi_territorial", "any").agg(
			F.sum("habitatges").alias("total_contracts"),
			(F.sum(F.col("renda_imputed") * F.col("habitatges")) / F.sum("habitatges")).alias("avg_monthly_rent")
		).withColumnRenamed("codi_territorial", "municipality_id")
		
		print("Processing RFDBC data...")
		df_rfdbc_pivoted = data['rfdbc'].groupBy("municipality_id", "year").pivot("indicator_code").agg(
			F.first("value")
		).withColumnRenamed("year", "any")
		
		df_rfdbc_pivoted = df_rfdbc_pivoted.withColumnRenamed("PER_CAPITA_EUR", "income_per_capita") \
										  .withColumnRenamed("PER_CAPITA_INDEX", "income_index_cat_100") \
										  .withColumnRenamed("VALUE_EK", "income_total_thousands_eur")

		print("Joining annual Lloguer and all RFDBC data...")
		df_annual_data = df_lloguer_annual.join(
			df_rfdbc_pivoted,
			["municipality_id", "any"],
			"full_outer"
		).filter(
			F.col("municipality_id").isNotNull() & F.col("any").isNotNull()
		)
		
		return {
			"idescat_data": data['idescat'],
			"annual_data": df_annual_data,
			**{k: v for k, v in data.items() if k.endswith('_neighbors') or k.endswith('_prov')}
		}

	def _populate_graph(self, processed_data: dict):
		"""Iterates over data to generate and add fully reified RDF triples to the graph."""
		print("\n--- Step 3: Populating the Knowledge Graph (Reified Model) ---")

		idescat_df = processed_data['idescat_data']
		annual_df = processed_data['annual_data']
		mun_neighbors = processed_data['mun_neighbors']
		com_neighbors = processed_data['com_neighbors']
		com_to_prov = processed_data['com_to_prov']
		
		created_nodes = set()

		print("Generating structural nodes and relationships...")
		geo_structure_df = idescat_df.select("municipality_id", "municipality_name", "comarca_name").distinct()
		for row in tqdm(geo_structure_df.collect(), desc="Creating Geo Entities"):
			mun_id, mun_name, com_name = row['municipality_id'], row['municipality_name'], row['comarca_name']
			if not mun_id or not com_name: continue

			mun_uri = PROJ[f"municipality/{mun_id}"]
			com_uri = PROJ[f"comarca/{com_name.replace(' ', '_')}"]
			
			if mun_uri not in created_nodes:
				self.graph.add((mun_uri, RDF.type, PROJ.Municipality))
				self.graph.add((mun_uri, RDFS.label, Literal(mun_name, lang='ca')))
				self.graph.add((mun_uri, PROJ.isInComarca, com_uri))
				created_nodes.add(mun_uri)

			if com_uri not in created_nodes:
				prov_name = com_to_prov.get(com_name)
				self.graph.add((com_uri, RDF.type, PROJ.Comarca))
				self.graph.add((com_uri, RDFS.label, Literal(com_name, lang='ca')))
				if prov_name:
					prov_uri = PROJ[f"province/{prov_name.replace(' ', '_')}"]
					self.graph.add((com_uri, PROJ.isInProvince, prov_uri))
					if prov_uri not in created_nodes:
						self.graph.add((prov_uri, RDF.type, PROJ.Province))
						self.graph.add((prov_uri, RDFS.label, Literal(prov_name, lang='ca')))
						created_nodes.add(prov_uri)
				created_nodes.add(com_uri)

		for mun_id, data in mun_neighbors.items():
			for neighbor in data['neighbors']:
				self.graph.add((PROJ[f"municipality/{mun_id}"], PROJ.isNeighborOf, PROJ[f"municipality/{neighbor['id']}"]))
		for com_name, data in com_neighbors.items():
			for neighbor_name in data['neighbors']:
				self.graph.add((PROJ[f"comarca/{com_name.replace(' ', '_')}"], PROJ.isAdjacentTo, PROJ[f"comarca/{neighbor_name.replace(' ', '_')}"]))

		print("Generating nodes for Idescat indicator concepts...")
		for indicator_name in self.indicator_map.values():
			indicator_uri = PROJ[f"indicator/{indicator_name}"]
			if indicator_uri not in created_nodes:
				self.graph.add((indicator_uri, RDF.type, PROJ.IdescatIndicator))
				self.graph.add((indicator_uri, RDFS.label, Literal(indicator_name)))
				created_nodes.add(indicator_uri)

		print("Generating Idescat latest-value observations...")
		window_spec = Window.partitionBy("municipality_id", "indicator_id").orderBy(F.col("reference_year").desc())
		idescat_latest_df = idescat_df.withColumn("rank", F.row_number().over(window_spec)).filter(F.col("rank") == 1).drop("rank")
		
		for row in tqdm(idescat_latest_df.collect(), desc="Creating Idescat Observations"):
			mun_id, indicator_id, value, ref_year = row['municipality_id'], row['indicator_id'], row['municipality_value'], row['reference_year']
			if any(v is None for v in [mun_id, indicator_id, value, ref_year]): continue

			indicator_name = self.indicator_map.get(indicator_id)
			if not indicator_name: continue

			obs_uri = PROJ[f"observation/{mun_id}_{indicator_name}"]
			mun_uri = PROJ[f"municipality/{mun_id}"]
			indicator_uri = PROJ[f"indicator/{indicator_name}"]

			self.graph.add((obs_uri, RDF.type, PROJ.IndicatorObservation))
			self.graph.add((obs_uri, RDFS.label, Literal(f"{indicator_name}_{mun_id}")))
			self.graph.add((obs_uri, PROJ.value, Literal(value, datatype=XSD.double)))
			self.graph.add((obs_uri, PROJ.referenceYear, Literal(ref_year, datatype=XSD.gYear)))
			self.graph.add((mun_uri, PROJ.hasObservation, obs_uri))
			self.graph.add((obs_uri, PROJ.field, indicator_uri))

		print("Generating AnnualDataPoint nodes with all annual data...")
		for row in tqdm(annual_df.toLocalIterator(), desc="Creating Annual Data Points"):
			mun_id = row['municipality_id']
			year = row['any']
			
			mun_uri = PROJ[f"municipality/{mun_id}"]
			datapoint_uri = PROJ[f"datapoint/{mun_id}_{int(year)}"]
			
			self.graph.add((datapoint_uri, RDF.type, PROJ.AnnualDataPoint))
			self.graph.add((mun_uri, PROJ.hasAnnualData, datapoint_uri))
			
			# Re-added the referenceYear property to the AnnualDataPoint
			self.graph.add((datapoint_uri, PROJ.referenceYear, Literal(int(year), datatype=XSD.gYear)))
			
			if row['avg_monthly_rent'] is not None and pd.notna(row['avg_monthly_rent']):
				self.graph.add((datapoint_uri, PROJ.avgMonthlyRent, Literal(row['avg_monthly_rent'], datatype=XSD.double)))
			if row['total_contracts'] is not None and pd.notna(row['total_contracts']):
				self.graph.add((datapoint_uri, PROJ.totalContracts, Literal(row['total_contracts'], datatype=XSD.integer)))
			if row['income_per_capita'] is not None and pd.notna(row['income_per_capita']):
				self.graph.add((datapoint_uri, PROJ.incomePerCapita, Literal(row['income_per_capita'], datatype=XSD.double)))
			if row['income_index_cat_100'] is not None and pd.notna(row['income_index_cat_100']):
				self.graph.add((datapoint_uri, PROJ.incomeIndex, Literal(row['income_index_cat_100'], datatype=XSD.double)))
			if row['income_total_thousands_eur'] is not None and pd.notna(row['income_total_thousands_eur']):
				self.graph.add((datapoint_uri, PROJ.incomeTotal, Literal(row['income_total_thousands_eur'], datatype=XSD.double)))

	def run(self):
		"""Executes the full Exploitation Zone pipeline."""
		try:
			raw_data = self._load_data()
			processed_data = self._process_data_for_kg(raw_data)
			self._populate_graph(processed_data)
			
			output_file = EXPLOITATION_DIR / "knowledge_graph.ttl"
			print(f"\n--- Pas 4: Saving Knowledge Graph to {output_file} ---")
			self.graph.serialize(destination=str(output_file), format='turtle')
			print(f"Graph saved successfully. Contains {len(self.graph)} triples.")
			print("\n--- Exploitation Zone (KG) Task Successfully Completed ---")
		except Exception as e:
			print(f"!!! ERROR during Exploitation Zone execution: {e}")
			traceback.print_exc()

if __name__ == "__main__":
	from spark_session import get_spark_session
	spark_session = None
	try:
		spark_session = get_spark_session()
		kg_processor = ExploitationZone(spark=spark_session)
		kg_processor.run()
	finally:
		if spark_session:
			print("\nStopping Spark Session.")
			spark_session.stop()