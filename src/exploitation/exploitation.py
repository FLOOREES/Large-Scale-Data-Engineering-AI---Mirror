import json
from pathlib import Path
import traceback
from tqdm import tqdm
import pandas as pd
from rdflib import Graph, Literal, Namespace
from rdflib.namespace import RDF, RDFS, XSD
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql import functions as F
from pyspark.sql.window import Window
import geo_relations

# --- Configuration ---
PROJ = Namespace("http://example.com/catalonia-ontology/")
TRUSTED_DIR = Path("./data/trusted/")
RELATIONS_DIR = Path("./data/relations/")
EXPLOITATION_DIR = Path("./data/exploitation/")

class ExploitationZone:
    """
    Orchestrates the creation of two key assets in the Exploitation Zone:
    1. A feature-rich, consolidated annual tabular dataset for ML tasks.
    2. A comprehensive Knowledge Graph for semantic queries and embeddings.
    """

    def __init__(self, spark: SparkSession):
        self.spark = spark
        geo_relations.main()
        self.graph = self._initialize_graph()
        self.indicator_map = self._define_indicator_map()
        print("Exploitation Zone Initialized.")

    def _initialize_graph(self) -> Graph:
        g = Graph()
        g.bind("proj", PROJ)
        g.bind("rdf", RDF)
        g.bind("rdfs", RDFS)
        g.bind("xsd", XSD)
        return g

    def _define_indicator_map(self) -> dict:
        return {
            'f171': 'population', 'f36': 'populationMen', 'f42': 'populationWomen',
            'f187': 'birthsTotal', 'f183': 'populationSpanishNat',
            'f261': 'surfaceKM2', 'f262': 'densityPopKM2',
            'f328': 'longitude', 'f329': 'latitude',
            'f308': 'unemploymentTotal', 'f191': 'totalFamilyDwellings',
            'f270': 'publicLibrariesCount', 'f293': 'sportsPavilionsCount',
            'f294': 'multiSportsCourtsCount', 'f301': 'indoorPoolsCount'
        }

    def _load_data(self) -> dict:
        print("\n--- Step 1: Loading all trusted data sources ---")
        data = {}
        try:
            data['idescat'] = self.spark.read.format("delta").load(str(TRUSTED_DIR / "idescat"))
            data['lloguer'] = self.spark.read.format("delta").load(str(TRUSTED_DIR / "lloguer"))
            data['rfdbc'] = self.spark.read.format("delta").load(str(TRUSTED_DIR / "rfdbc"))
            with open(RELATIONS_DIR / "municipality_neighbors.json", 'r') as f: data['mun_neighbors'] = json.load(f)
            with open(RELATIONS_DIR / "comarca_neighbors.json", 'r') as f: data['com_neighbors'] = json.load(f)
            with open(RELATIONS_DIR / "comarca_to_province.json", 'r') as f: data['com_to_prov'] = json.load(f)
            print("All trusted data and relation files loaded successfully.")
            return data
        except Exception as e:
            print(f"ERROR: Could not load one or more data sources. Details: {e}")
            raise

    def _create_ml_table(self, data: dict) -> DataFrame:
        """Creates the feature-rich consolidated table for the ML prediction task."""
        print("\n--- Step 2a: Creating Consolidated Annual Table for ML ---")
        df_lloguer = data['lloguer'].filter(F.col("ambit_territorial") == "Municipi").withColumn(
            "renda_imputed", F.when(F.col("renda").isNotNull(), F.col("renda"))
             .when(F.col("tram_preus") == "<= 350 euros/mes", 325.0).when(F.col("tram_preus") == "> 350 i <= 450 euros/mes", 400.0)
             .when(F.col("tram_preus") == "> 350 i <= 500 euros/mes", 425.0).when(F.col("tram_preus") == "> 450 i <= 600 euros/mes", 525.0)
             .when(F.col("tram_preus") == "> 500 i <= 650 euros/mes", 575.0).when(F.col("tram_preus") == "> 600 euros/mes", 800.0)
             .when(F.col("tram_preus") == "> 650 euros/mes", 900.0).otherwise(None))
        df_lloguer_annual = df_lloguer.groupBy("codi_territorial", "any").agg(
            F.first("nom_territori").alias("municipality_name"),
            F.sum("habitatges").alias("total_contracts"),
            (F.sum(F.col("renda_imputed") * F.col("habitatges")) / F.sum("habitatges")).alias("avg_monthly_rent")
        ).withColumnRenamed("codi_territorial", "municipality_id")

        df_rfdbc_processed = data['rfdbc'].filter(F.col("indicator_code") == "PER_CAPITA_EUR").select(
            F.col("municipality_id"), F.col("year").cast("int").alias("any"), F.col("value").alias("income_per_capita"))
        df_merged_annual = df_lloguer_annual.join(df_rfdbc_processed, ["municipality_id", "any"], "full_outer")

        window_spec = Window.partitionBy("municipality_id", "indicator_id").orderBy(F.col("reference_year").desc())
        df_idescat_latest = data['idescat'].withColumn("rank", F.row_number().over(window_spec)).filter(F.col("rank") == 1).drop("rank")
        df_idescat_pivoted = df_idescat_latest.groupBy("municipality_id", "comarca_name").pivot("indicator_id", list(self.indicator_map.keys())).agg(F.first("municipality_value"))
        for old_name, new_name in self.indicator_map.items():
            if old_name in df_idescat_pivoted.columns:
                df_idescat_pivoted = df_idescat_pivoted.withColumnRenamed(old_name, new_name)
        
        df_final = df_merged_annual.join(F.broadcast(df_idescat_pivoted), on="municipality_id", how="left")
        return df_final

    def _prepare_data_for_kg(self, data: dict) -> dict:
        """Processes and joins data exactly as it was done for the original KG."""
        print("\n--- Step 2b: Preparing Data for Knowledge Graph Population ---")
        df_lloguer_imputed = data['lloguer'].filter(F.col("ambit_territorial") == "Municipi").withColumn(
			"renda_imputed", F.when(F.col("renda").isNotNull(), F.col("renda"))
			 .when(F.col("tram_preus") == "<= 350 euros/mes", 325.0).when(F.col("tram_preus") == "> 350 i <= 450 euros/mes", 400.0)
			 .when(F.col("tram_preus") == "> 350 i <= 500 euros/mes", 425.0).when(F.col("tram_preus") == "> 450 i <= 600 euros/mes", 525.0)
			 .when(F.col("tram_preus") == "> 500 i <= 650 euros/mes", 575.0).when(F.col("tram_preus") == "> 600 euros/mes", 800.0)
			 .when(F.col("tram_preus") == "> 650 euros/mes", 900.0).otherwise(None))
        df_lloguer_annual = df_lloguer_imputed.groupBy("codi_territorial", "any").agg(
			F.sum("habitatges").alias("total_contracts"),
			(F.sum(F.col("renda_imputed") * F.col("habitatges")) / F.sum("habitatges")).alias("avg_monthly_rent")
		).withColumnRenamed("codi_territorial", "municipality_id")
        
        df_rfdbc_pivoted = data['rfdbc'].groupBy("municipality_id", "year").pivot("indicator_code").agg(F.first("value")).withColumnRenamed("year", "any")
        df_rfdbc_pivoted = df_rfdbc_pivoted.withColumnRenamed("PER_CAPITA_EUR", "income_per_capita") \
										  .withColumnRenamed("PER_CAPITA_INDEX", "income_index_cat_100") \
										  .withColumnRenamed("VALUE_EK", "income_total_thousands_eur")
        
        df_annual_data = df_lloguer_annual.join(
			df_rfdbc_pivoted, ["municipality_id", "any"], "full_outer"
		).filter(F.col("municipality_id").isNotNull() & F.col("any").isNotNull())
        return {"idescat_data": data['idescat'], "annual_data": df_annual_data, **{k: v for k, v in data.items() if k.endswith('_neighbors') or k.endswith('_prov')}}

    def _populate_graph(self, processed_data: dict):
        print("\n--- Step 3: Populating the Knowledge Graph ---")
        idescat_df, annual_df = processed_data['idescat_data'], processed_data['annual_data']
        mun_neighbors, com_neighbors, com_to_prov = processed_data['mun_neighbors'], processed_data['com_neighbors'], processed_data['com_to_prov']
        created_nodes = set()
        
        print("Generating structural nodes and relationships...")
        geo_structure_df = idescat_df.select("municipality_id", "municipality_name", "comarca_name").distinct()
        for row in tqdm(geo_structure_df.collect(), desc="Creating Geo Entities"):
            mun_id, mun_name, com_name = row['municipality_id'], row['municipality_name'], row['comarca_name']
            if not mun_id or not com_name: continue
            mun_uri, com_uri = PROJ[f"municipality/{mun_id}"], PROJ[f"comarca/{com_name.replace(' ', '_')}"]
            if mun_uri not in created_nodes:
                self.graph.add((mun_uri, RDF.type, PROJ.Municipality)); self.graph.add((mun_uri, RDFS.label, Literal(mun_name, lang='ca'))); self.graph.add((mun_uri, PROJ.isInComarca, com_uri)); created_nodes.add(mun_uri)
            if com_uri not in created_nodes:
                prov_name = com_to_prov.get(com_name)
                self.graph.add((com_uri, RDF.type, PROJ.Comarca)); self.graph.add((com_uri, RDFS.label, Literal(com_name, lang='ca')))
                if prov_name:
                    prov_uri = PROJ[f"province/{prov_name.replace(' ', '_')}"]
                    self.graph.add((com_uri, PROJ.isInProvince, prov_uri))
                    if prov_uri not in created_nodes: self.graph.add((prov_uri, RDF.type, PROJ.Province)); self.graph.add((prov_uri, RDFS.label, Literal(prov_name, lang='ca'))); created_nodes.add(prov_uri)
                created_nodes.add(com_uri)
        for mun_id, data in mun_neighbors.items():
            for neighbor in data['neighbors']: self.graph.add((PROJ[f"municipality/{mun_id}"], PROJ.isNeighborOf, PROJ[f"municipality/{neighbor['id']}"]))
        for com_name, data in com_neighbors.items():
            for neighbor_name in data['neighbors']: self.graph.add((PROJ[f"comarca/{com_name.replace(' ', '_')}"], PROJ.isAdjacentTo, PROJ[f"comarca/{neighbor_name.replace(' ', '_')}"]))

        print("Generating nodes for Idescat indicator concepts...")
        for indicator_name in self.indicator_map.values():
            indicator_uri = PROJ[f"indicator/{indicator_name}"]
            if indicator_uri not in created_nodes: self.graph.add((indicator_uri, RDF.type, PROJ.IdescatIndicator)); self.graph.add((indicator_uri, RDFS.label, Literal(indicator_name))); created_nodes.add(indicator_uri)

        print("Generating Idescat latest-value observations...")
        window_spec = Window.partitionBy("municipality_id", "indicator_id").orderBy(F.col("reference_year").desc())
        idescat_latest_df = idescat_df.withColumn("rank", F.row_number().over(window_spec)).filter(F.col("rank") == 1).drop("rank")
        for row in tqdm(idescat_latest_df.collect(), desc="Creating Idescat Observations"):
            mun_id, indicator_id, value, ref_year = row['municipality_id'], row['indicator_id'], row['municipality_value'], row['reference_year']
            if any(v is None for v in [mun_id, indicator_id, value, ref_year]): continue
            indicator_name = self.indicator_map.get(indicator_id)
            if not indicator_name: continue
            obs_uri, mun_uri, indicator_uri = PROJ[f"observation/{mun_id}_{indicator_name}"], PROJ[f"municipality/{mun_id}"], PROJ[f"indicator/{indicator_name}"]
            self.graph.add((obs_uri, RDF.type, PROJ.IndicatorObservation)); self.graph.add((obs_uri, RDFS.label, Literal(f"{mun_id}_{indicator_name}"))); self.graph.add((obs_uri, PROJ.value, Literal(value, datatype=XSD.double))); self.graph.add((obs_uri, PROJ.referenceYear, Literal(ref_year, datatype=XSD.gYear))); self.graph.add((mun_uri, PROJ.hasObservation, obs_uri)); self.graph.add((obs_uri, PROJ.field, indicator_uri))

        print("Generating AnnualDataPoint nodes with all annual data...")
        for row in tqdm(annual_df.toLocalIterator(), desc="Creating Annual Data Points"):
            mun_id, year = row['municipality_id'], row['any']
            if not mun_id or pd.isna(year): continue
            mun_uri, datapoint_uri = PROJ[f"municipality/{mun_id}"], PROJ[f"datapoint/{mun_id}_{int(year)}"]
            self.graph.add((datapoint_uri, RDF.type, PROJ.AnnualDataPoint)); self.graph.add((mun_uri, PROJ.hasAnnualData, datapoint_uri)); self.graph.add((datapoint_uri, PROJ.referenceYear, Literal(int(year), datatype=XSD.gYear)))
            if 'avg_monthly_rent' in row and pd.notna(row['avg_monthly_rent']): self.graph.add((datapoint_uri, PROJ.avgMonthlyRent, Literal(row['avg_monthly_rent'], datatype=XSD.double)))
            if 'total_contracts' in row and pd.notna(row['total_contracts']): self.graph.add((datapoint_uri, PROJ.totalContracts, Literal(row['total_contracts'], datatype=XSD.integer)))
            if 'income_per_capita' in row and pd.notna(row['income_per_capita']): self.graph.add((datapoint_uri, PROJ.incomePerCapita, Literal(row['income_per_capita'], datatype=XSD.double)))
            if 'income_index_cat_100' in row and pd.notna(row['income_index_cat_100']): self.graph.add((datapoint_uri, PROJ.incomeIndex, Literal(row['income_index_cat_100'], datatype=XSD.double)))
            if 'income_total_thousands_eur' in row and pd.notna(row['income_total_thousands_eur']): self.graph.add((datapoint_uri, PROJ.incomeTotal, Literal(row['income_total_thousands_eur'], datatype=XSD.double)))
        
    def run(self):
        try:
            all_data = self._load_data()
            
            # Flow 1: Create and save the rich table for the ML task
            df_for_ml = self._create_ml_table(all_data)
            table_output_path = EXPLOITATION_DIR / "municipal_annual"
            print(f"\n--- Saving consolidated annual table to: {table_output_path} ---")
            df_for_ml.write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(str(table_output_path))
            print("Consolidated table for ML saved successfully.")
            
            # Flow 2: Prepare the original data and populate the KG to ensure it's identical
            data_for_kg = self._prepare_data_for_kg(all_data)
            self._populate_graph(data_for_kg)
            
            kg_output_file = EXPLOITATION_DIR / "knowledge_graph.ttl"
            print(f"\n--- Saving Knowledge Graph to {kg_output_file} ---")
            self.graph.serialize(destination=str(kg_output_file), format='turtle')
            print(f"Knowledge Graph saved successfully. Contains {len(self.graph)} triples.")
            
            print("\n--- Exploitation Zone (KG & ML Table) Task Successfully Completed ---")
        except Exception as e:
            print(f"!!! ERROR during Exploitation Zone execution: {e}")
            traceback.print_exc()

if __name__ == "__main__":
    from spark_session import get_spark_session
    spark = None
    try:
        spark = get_spark_session()
        processor = ExploitationZone(spark=spark)
        processor.run()
    except Exception as main_error:
        print(f"\nAn unexpected error occurred in the main execution block: {main_error}")
        traceback.print_exc()
    finally:
        if spark:
            print("\nStopping Spark Session.")
            spark.stop()
            print("Spark Session stopped.")