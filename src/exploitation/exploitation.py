from pathlib import Path
from typing import Optional, Dict
import traceback

# --- PySpark Imports ---
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql import functions as F
from pyspark.sql.window import Window
from pyspark.sql.types import (
	StructType, StructField, StringType, IntegerType, DoubleType, LongType
)
# ---------------------

class ExploitationZone:
	"""
	Merges and prepares data from Trusted Zone tables (Lloguer, RFDBC, Idescat)
	into a consolidated annual, municipality-level Delta table for downstream analysis.
	Pivots the LATEST AVAILABLE value for each selected Idescat indicator per municipality,
	regardless of the reference year.
	"""

	def __init__(self, spark: SparkSession,
				 lloguer_trusted_path: str = "./data/trusted/lloguer",
				 rfdbc_trusted_path: str = "./data/trusted/rfdbc",
				 idescat_trusted_path: str = "./data/trusted/idescat",
				 output_path: str = "./data/exploitation"):
		self.spark = spark
		self.lloguer_path = str(lloguer_trusted_path)
		self.rfdbc_path = str(rfdbc_trusted_path)
		self.idescat_path = str(idescat_trusted_path)
		self.output_path = str(output_path)
		print(f"Exploitation Zone Initialized.")
		print(f"  Input Lloguer: {self.lloguer_path}")
		print(f"  Input RFDBC (Salary): {self.rfdbc_path}")
		print(f"  Input Idescat: {self.idescat_path}")
		print(f"  Output Consolidated Path: {self.output_path}")

	def _load_trusted_data(self) -> Dict[str, DataFrame]:
		"""Loads the three trusted Delta tables."""
		print("\n--- Loading Trusted Data ---")
		dataframes = {}
		paths = {
			"lloguer": self.lloguer_path,
			"rfdbc": self.rfdbc_path,
			"idescat": self.idescat_path
		}
		for name, path in paths.items():
			try:
				print(f"Loading {name} data from: {path}")
				df = self.spark.read.format("delta").load(path)
				print(f"Loaded {name} data successfully ({df.count()} rows). Schema:")
				df.printSchema()
				dataframes[name] = df
			except Exception as e:
				print(f"ERROR: Failed to load trusted data for '{name}' from {path}. Error: {e}")
				raise RuntimeError(f"Could not load input data for {name}") from e
		return dataframes

	def _process_lloguer(self, df_lloguer: DataFrame) -> DataFrame:
		"""
		Filters, imputes missing 'renda' using 'tram_preus', and aggregates
		Lloguer data annually per municipality.
		"""
		print("\n--- Processing Lloguer Data (Impute Missing Rent & Annual Aggregation) ---")
		# Identify columns safely
		id_col = next((col for col in ["codi_territorial", "municipality_id"] if col in df_lloguer.columns), None)
		name_col = next((col for col in ["nom_territori", "municipality_name"] if col in df_lloguer.columns), None)
		if not id_col or not name_col:
			raise ValueError("Could not find expected ID or Name columns in Lloguer DataFrame.")

		# Filter for municipalities
		df_lloguer_filtered = df_lloguer.filter(F.col("ambit_territorial") == "Municipi")
		count_filt = df_lloguer_filtered.count()
		print(f"Filtered Lloguer for 'Municipi', rows remaining: {count_filt}")
		unique_categories = df_lloguer_filtered.select("tram_preus").distinct().orderBy("tram_preus")
		print("Unique 'tram_preus' categories:")
		unique_categories.show(truncate=False)
		if count_filt == 0:
			print("Warning: No Lloguer data for 'Municipi'. Returning empty DF.")
			# Define schema matching the expected output
			schema = StructType([
				 StructField("municipality_id", StringType(), True), StructField("municipality_name", StringType(), True),
				 StructField("any", IntegerType(), True), StructField("avg_monthly_rent_eur", DoubleType(), True),
				 StructField("total_annual_contracts", LongType(), True)
			 ])
			return self.spark.createDataFrame([], schema)

		# ** Impute missing 'renda' using 'tram_preus' **
		print("Imputing missing 'renda' values using 'tram_preus' midpoints...")
		df_lloguer_imputed = df_lloguer_filtered.withColumn(
			"renda_imputed",
			F.when(F.col("renda").isNotNull(), F.col("renda")) # Keep existing value if not null
			 .when(F.col("tram_preus") == "<= 350 euros/mes", 325.0) # Estimate for lowest bracket
			 .when(F.col("tram_preus") == "> 350 i <= 450 euros/mes", 400.0) # Midpoint
			 .when(F.col("tram_preus") == "> 350 i <= 500 euros/mes", 425.0) # Midpoint
			 .when(F.col("tram_preus") == "> 450 i <= 600 euros/mes", 525.0) # Midpoint
			 .when(F.col("tram_preus") == "> 500 i <= 650 euros/mes", 575.0) # Midpoint
			 .when(F.col("tram_preus") == "> 600 euros/mes", 800) # Estimate for highest bracket
			 .when(F.col("tram_preus") == "> 650 euros/mes", 900) # Estimate for highest bracket
			 .otherwise(None) # If tram_preus doesn't match, leave null
		)

		# Check how many values were imputed (optional)
		imputed_count = df_lloguer_imputed.filter(F.col("renda").isNull() & F.col("renda_imputed").isNotNull()).count()
		print(f"Imputed {imputed_count} missing 'renda' values based on 'tram_preus'.")

		# Calculate weighted imputed rent
		# Use coalesce(renda_imputed, 0.0) if you want to treat rows with unmatchable tram_preus as having 0 contribution to weighted avg
		# Or filter them out beforehand: df_lloguer_imputed = df_lloguer_imputed.filter(F.col("renda_imputed").isNotNull())
		# For now, let's proceed assuming imputation covers most/all cases or nulls are acceptable
		df_lloguer_weighted = df_lloguer_imputed.withColumn(
			"weighted_renda",
			 # Multiply imputed rent by habitatges. Handle potential nulls in imputed value if needed.
			F.when(F.col("renda_imputed").isNotNull(), F.col("renda_imputed") * F.col("habitatges"))
			 .otherwise(0.0) # Assign 0 weight if imputed rent is null
		)

		# Aggregate by municipality code and year
		print("Aggregating imputed data annually...")
		df_lloguer_annual = df_lloguer_weighted.groupBy(id_col, "any").agg(
			F.first(name_col).alias("municipality_name"),
			F.sum("habitatges").alias("total_annual_contracts"),
			F.sum("weighted_renda").alias("total_weighted_renda") # Sum of weighted *imputed* rents
		).withColumn(
			 # Calculate weighted average *imputed* rent
			"avg_monthly_rent_eur",
			F.when(F.col("total_annual_contracts") > 0, F.col("total_weighted_renda") / F.col("total_annual_contracts"))
			.otherwise(None)
		).withColumnRenamed(id_col, "municipality_id") # Standardize ID column name

		# Select and order final columns
		df_result = df_lloguer_annual.select(
			"municipality_id",
			"municipality_name",
			"any",
			"avg_monthly_rent_eur", # This now reflects the average based on imputed values
			"total_annual_contracts"
		)
		print("Lloguer data imputed and aggregated annually. Schema:")
		df_result.printSchema()
		df_result.show(5, truncate=False)
		return df_result

	def _process_rfdbc_salary(self, df_rfdbc: DataFrame) -> DataFrame:
		"""Filters and prepares Salary data (RFDBC), including municipality name."""
		print("\n--- Processing RFDBC Data (Extracting Salary) ---")
		salary_indicator = "PER_CAPITA_EUR"
		df_salary_filtered = df_rfdbc.filter(F.col("indicator_code") == salary_indicator)
		count_filt = df_salary_filtered.count()
		print(f"Filtered RFDBC for indicator_code='{salary_indicator}', rows remaining: {count_filt}")
		if count_filt == 0:
			print("Warning: No RFDBC data for salary indicator. Returning empty DF.")
			schema = StructType([
				 StructField("municipality_id", StringType(), True), StructField("municipality_name", StringType(), True),
				 StructField("any", IntegerType(), True), StructField("salary_per_capita_eur", DoubleType(), True)
			 ])
			return self.spark.createDataFrame([], schema)
		df_result = df_salary_filtered.select(
			F.col("municipality_id"), F.col("municipality_name"),
			F.col("year").cast(IntegerType()).alias("any"),
			F.col("value").alias("salary_per_capita_eur")
		)
		print("Salary data prepared. Schema:")
		df_result.printSchema()
		df_result.show(5, truncate=False)
		return df_result

	# *** MODIFIED Idescat Processing to get LATEST VALUE PER INDICATOR ***
	def _process_idescat_indicators(self, df_idescat: DataFrame) -> Optional[DataFrame]:
		"""
		Finds the latest available value for each desired indicator per municipality,
		keeps names, and pivots the result. Mixes reference years.
		"""
		print("\n--- Processing Idescat Data (Pivoting LATEST AVAILABLE Value per Indicator) ---")
		try:
			required_cols = ["reference_year", "municipality_id", "municipality_name", "comarca_name", "indicator_id", "municipality_value"]
			if not all(col in df_idescat.columns for col in required_cols):
				missing = [col for col in required_cols if col not in df_idescat.columns]
				print(f"ERROR: Idescat input missing required columns: {missing}. Skipping.")
				return None
		except Exception as e:
			print(f"ERROR checking Idescat columns: {e}. Skipping.")
			return None

		# Define ALL potentially desired indicators and their final names
		all_desired_indicators = {
			'f171': 'population', 'f36':  'population_men', 'f42':  'population_women',
			'f187': 'births_total', 'f183': 'population_spanish_nat',
			'f261': 'surface_km2', 'f262': 'density_pop_km2',
			'f328': 'longitude', 'f329': 'latitude',
			'f308': 'unemployment_total_avg', 'f191': 'total_family_dwellings',
			'f270': 'public_libraries_count', 'f293': 'sports_pavilions_count',
			'f294': 'multi_sports_courts_count', 'f301': 'indoor_pools_count'
			# Add f271 (Income) and f279 (Businesses) if needed
		}
		all_desired_ids = list(all_desired_indicators.keys())

		# Filter for only the desired indicators from the entire trusted table
		df_filtered_indicators = df_idescat.filter(F.col("indicator_id").isin(all_desired_ids))
		count_filtered = df_filtered_indicators.count()
		print(f"Filtered Idescat for desired indicators {all_desired_ids}, rows remaining: {count_filtered}")
		if count_filtered == 0:
			print(f"Warning: No data found for any desired indicators in trusted Idescat data. Skipping.")
			return None

		# --- Use Window Function to find the latest value for each indicator per municipality ---
		print("Finding latest value for each indicator per municipality...")
		window_spec = Window.partitionBy("municipality_id", "indicator_id") \
							  .orderBy(F.col("reference_year").desc())

		df_latest_per_indicator = df_filtered_indicators.withColumn(
			"rank", F.row_number().over(window_spec)
		).filter(F.col("rank") == 1).drop("rank") # Keep only the latest row for each group
		# ------------------------------------------------------------------------------------

		print(f"Pivoting latest available indicators...")
		# Pivot the table containing latest values. Group by ID AND Names.
		df_pivoted = df_latest_per_indicator.groupBy(
			"municipality_id", "municipality_name", "comarca_name" # Keep names
		).pivot(
			"indicator_id", all_desired_ids # Pivot on ALL desired IDs
		).agg(
			# We kept only rank=1 rows, so first municipality_value is the latest one
			F.first("municipality_value")
		)

		print("Renaming pivoted indicator columns...")
		df_renamed = df_pivoted
		# Rename columns that were actually created by the pivot
		for old_name, new_name in all_desired_indicators.items():
			if old_name in df_renamed.columns:
				df_renamed = df_renamed.withColumnRenamed(old_name, new_name)
			else:
				# If an indicator *never* existed in the trusted data, add it as null
				print(f"  - Note: Indicator '{old_name}' not found in any year. Adding '{new_name}' as null.")
				df_renamed = df_renamed.withColumn(new_name, F.lit(None).cast(DoubleType()))

		df_result = df_renamed

		print("Idescat indicators pivoted (latest available per indicator). Schema:")
		df_result.printSchema()
		df_result.show(5, truncate=False)
		return df_result

	def _join_data(self, df_lloguer: DataFrame, df_salary: DataFrame, df_indicators: Optional[DataFrame]) -> Optional[DataFrame]:
		"""Joins the processed Lloguer, Salary, and (latest available) Indicator data."""
		print("\n--- Joining Datasets ---")
		df_lloguer = df_lloguer.alias("llog")
		df_salary = df_salary.alias("sal")
		print("Joining Lloguer (annual) and Salary (annual)...")
		df_merged = df_lloguer.join(
			df_salary,
			(df_lloguer["municipality_id"] == df_salary["municipality_id"]) & \
			(df_lloguer["any"] == df_salary["any"]),
			how="full_outer"
		).select(
			F.coalesce(df_lloguer["municipality_id"], df_salary["municipality_id"]).alias("municipality_id"),
			F.coalesce(df_lloguer["any"], df_salary["any"]).alias("any"),
			F.coalesce(df_lloguer["municipality_name"], df_salary["municipality_name"]).alias("municipality_name"),
			df_lloguer["avg_monthly_rent_eur"],
			df_lloguer["total_annual_contracts"],
			df_salary["salary_per_capita_eur"]
		)
		count_merged = df_merged.count()
		print(f"Rows after Lloguer/Salary join: {count_merged}")
		if count_merged == 0:
			print("Warning: Join between Lloguer and Salary resulted in zero rows.")
			return None
		print("Schema after Lloguer/Salary join:")
		df_merged.printSchema()

		if df_indicators is not None:
			print("Joining with latest available Idescat indicators...")
			df_indicators = df_indicators.alias("ind")
			# Select all indicator columns EXCEPT the join key to avoid duplicates
			# Also exclude municipality_name as we already have a definitive one from df_merged
			indicator_cols_to_select = [df_indicators["municipality_id"], df_indicators["comarca_name"]]
			pivoted_indicator_cols = [col for col in df_indicators.columns if col not in ["municipality_id", "municipality_name", "comarca_name"]]
			indicator_cols_to_select.extend([df_indicators[col] for col in pivoted_indicator_cols])
			df_indicators_selected = df_indicators.select(indicator_cols_to_select)
			print("Schema of indicators being joined:")
			df_indicators_selected.printSchema()

			df_final = df_merged.join(
				F.broadcast(df_indicators_selected),
				df_merged["municipality_id"] == df_indicators_selected["municipality_id"],
				how="left"
			).drop(df_indicators_selected["municipality_id"]) # Drop duplicate key
			print(f"Rows after Indicator join: {df_final.count()}")
		else:
			# Handle case where indicators weren't processed
			print("Skipping indicator join as indicator data was not processed.")
			df_final = df_merged
			print("Adding NULL columns for missing indicator data...")
			# Add null columns for ALL potentially pivoted indicators
			indicator_placeholder_cols = {
				'comarca_name': StringType(), 'population': DoubleType(), 'population_men': DoubleType(),
				'population_women': DoubleType(), 'births_total': DoubleType(), 'population_spanish_nat': DoubleType(),
				'surface_km2': DoubleType(), 'density_pop_km2': DoubleType(), 'longitude': DoubleType(),
				'latitude': DoubleType(), 'unemployment_total_avg': DoubleType(), 'total_family_dwellings': DoubleType(),
				'public_libraries_count': DoubleType(), 'sports_pavilions_count': DoubleType(),
				'multi_sports_courts_count': DoubleType(), 'indoor_pools_count': DoubleType()
			}
			for col_name, col_type in indicator_placeholder_cols.items():
				if col_name not in df_final.columns:
					df_final = df_final.withColumn(col_name, F.lit(None).cast(col_type))

		print("Final joined schema (before final selection):")
		df_final.printSchema()
		return df_final

	def _inspect_output_delta(self):
		"""Reads the final Exploitation Delta table and prints info."""
		# (Keep inspection logic as before)
		print(f"\n--- Inspecting Output Exploitation Delta Table: {self.output_path} ---")
		try:
			delta_log_path = Path(self.output_path) / "_delta_log"
			if not delta_log_path.is_dir():
				print(f"Inspection failed: Delta log not found at {delta_log_path}.")
				return
			df_read_back = self.spark.read.format("delta").load(self.output_path)
			count = df_read_back.count()
			print(f"Successfully read back {count} records.")
			print("\nSchema in Exploitation Delta Table:")
			df_read_back.printSchema()
			print(f"\nSample Data (First 10 rows):")
			df_read_back.show(10, truncate=False)
		except Exception as e:
			print(f"!!! Inspection failed: Error reading Exploitation Delta table: {e}")
			traceback.print_exc()
		print(f"--- Finished Inspecting Output ---")

	def run(self, verbose: bool = False):
		"""Executes the full Exploitation Zone processing pipeline."""
		print(f"--- Running Exploitation Zone Task (Verbose: {verbose}) ---")
		write_successful = False
		try:
			# 1. Load Data
			trusted_data = self._load_trusted_data()
			df_lloguer_trusted = trusted_data.get("lloguer")
			df_rfdbc_trusted = trusted_data.get("rfdbc")
			df_idescat_trusted = trusted_data.get("idescat")
			if not all([df_lloguer_trusted, df_rfdbc_trusted, df_idescat_trusted]):
				print("Error: Not all trusted dataframes were loaded. Aborting.")
				return

			# 2. Process each dataset individually
			df_lloguer_processed = self._process_lloguer(df_lloguer_trusted)
			df_salary_processed = self._process_rfdbc_salary(df_rfdbc_trusted)
			# This now gets latest available per indicator
			df_indicators_processed = self._process_idescat_indicators(df_idescat_trusted)

			# 3. Join processed datasets
			df_consolidated = self._join_data(
				df_lloguer_processed,
				df_salary_processed,
				df_indicators_processed
			)
			if df_consolidated is None:
				print("Error: Data joining failed or resulted in empty data. Aborting write.")
				return

			# 4. Final Selection & Ordering - Define FULL list of columns expected
			final_columns = [
				"municipality_id", "municipality_name", "comarca_name",
				"any", # Year from Rent/Salary
				"avg_monthly_rent_eur", # Corrected name
				"total_annual_contracts", # Corrected name
				"salary_per_capita_eur",
				# Pivoted indicator names - list all desired ones
				"population", "population_men", "population_women",
				"births_total", "population_spanish_nat",
				"surface_km2", "density_pop_km2",
				"longitude", "latitude",
				"unemployment_total_avg",
				"total_family_dwellings",
				"public_libraries_count",
				"sports_pavilions_count",
				"multi_sports_courts_count",
				"indoor_pools_count"
			]
			# Select only the columns that actually EXIST after processing/joins
			existing_final_columns = [col for col in final_columns if col in df_consolidated.columns]
			if not existing_final_columns:
				print("ERROR: No columns selected for the final DataFrame.")
				raise ValueError("Final column selection resulted in empty list.")

			print(f"Final columns selected for output: {existing_final_columns}")
			df_final = df_consolidated.select(existing_final_columns)

			print("\n--- Final Consolidated Data ---")
			print("Schema before save:")
			df_final.printSchema()
			print("Sample final data:")
			df_final.show(10, truncate=False)


			print("\n--- Checking for rows with NULL Rent Data ---")
			null_rent_check_col = "avg_monthly_rent_eur" # Ensure this matches the final column name
			if null_rent_check_col in df_final.columns:
				null_rent_df = df_final.filter(F.col(null_rent_check_col).isNull())
				null_rent_count = null_rent_df.count()
				print(f"Number of rows with NULL '{null_rent_check_col}': {null_rent_count}")
				if null_rent_count > 0 and verbose:
					print(f"Showing up to 10 rows where '{null_rent_check_col}' is NULL:")
					# Show relevant columns to identify these rows
					null_rent_df.select("municipality_id", "municipality_name", "any", "salary_per_capita_eur", "avg_monthly_rent_eur").show(10, truncate=False)
			else:
				print(f"Warning: Column '{null_rent_check_col}' not found for null check.")

			# 5. Write to Exploitation Zone Delta Lake
			print(f"\nWriting consolidated data to Delta table: {self.output_path}")
			df_final.write.format("delta") \
				.mode("overwrite") \
				.option("overwriteSchema", "true") \
				.save(self.output_path)
			write_successful = True
			print(f"Consolidated data successfully written to {self.output_path}")

			if verbose and write_successful:
				self._inspect_output_delta()

			print("--- Exploitation Zone Task Successfully Completed ---")

		except Exception as e:
			print(f"!!! Error during Exploitation Zone execution: {e}")
			traceback.print_exc()
			print("--- Exploitation Zone Task Failed ---")

# --- Main Execution Block ---
if __name__ == "__main__":
	from src.spark_session import get_spark_session

	spark = None
	try:
		spark = get_spark_session()
		processor = ExploitationZone(
			spark=spark,
		)
		processor.run(verbose=True)

	except Exception as main_error:
		print(f"\nAn unexpected error occurred in the main execution block: {main_error}")
		traceback.print_exc()
	finally:
		if spark:
			print("\nStopping Spark Session.")
			spark.stop()
			print("Spark Session stopped.")