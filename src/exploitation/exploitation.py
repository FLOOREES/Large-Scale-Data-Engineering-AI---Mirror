# src/exploitation/exploitation.py

import json
from pathlib import Path
import traceback
from typing import Dict, Any

# --- PySpark Imports ---
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# --- RDFLib Imports ---
from rdflib import Graph, Namespace, URIRef, Literal
from rdflib.namespace import RDF, RDFS, XSD

# --- tqdm for progress bars ---
from tqdm import tqdm

class ExploitationZone:
    """
    Reads data from the Trusted Zone and geographic relation files to build
    an integrated Knowledge Graph (KG) of Catalan socio-economic and
    geographic data. The final output is a Turtle (.ttl) file.
    """

    def __init__(self, spark: SparkSession,
                 idescat_path: str = "./data/trusted/idescat",
                 lloguer_path: str = "./data/trusted/lloguer",
                 rfdbc_path: str = "./data/trusted/rfdbc",
                 mun_neighbors_path: str = "./data/relations/municipality_neighbors.json",
                 com_neighbors_path: str = "./data/relations/comarca_neighbors.json",
                 com_to_prov_path: str = "./data/relations/comarca_to_province.json",
                 output_kg_path: str = "./data/exploitation/knowledge_graph.ttl"):
        """Initializes the KG creation pipeline."""
        self.spark = spark
        # Input paths
        self.idescat_path = idescat_path
        self.lloguer_path = lloguer_path
        self.rfdbc_path = rfdbc_path
        self.mun_neighbors_path = mun_neighbors_path
        self.com_neighbors_path = com_neighbors_path
        self.com_to_prov_path = com_to_prov_path
        # Output path
        self.output_kg_path = Path(output_kg_path)
        self.output_kg_path.parent.mkdir(parents=True, exist_ok=True)

        # Initialize RDF Graph and Namespaces
        self.graph = Graph()
        self._define_namespaces_and_bind()

        # Desired Idescat indicators for static properties and their property names
        self.static_indicator_map = {
            'f171': 'population',
            'f261': 'surfaceAreaKM2',
            'f262': 'densityPopKM2',
            'f308': 'unemploymentTotal',
            'f329': 'latitude',
            'f328': 'longitude',
            'f36': 'populationMen',
            'f42': 'populationWomen',
            'f187': 'birthsTotal',
        }
        print("KnowledgeGraphExploitationZone Initialized.")

    def _define_namespaces_and_bind(self):
        """Defines and binds namespaces to the RDF graph."""
        self.PROJ = Namespace("http://example.com/catalonia-ontology/")
        self.graph.bind("proj", self.PROJ)
        self.graph.bind("rdf", RDF)
        self.graph.bind("rdfs", RDFS)
        self.graph.bind("xsd", XSD)
        print("Namespaces defined and bound to the graph.")

    def _load_data(self) -> Dict[str, Any]:
        """Loads all data sources: Delta tables into Spark and JSONs into Python dicts."""
        print("\n--- Step 1: Loading all data sources ---")
        loaded_data = {}
        try:
            # Load Delta tables
            print("Loading trusted Delta tables...")
            loaded_data['idescat'] = self.spark.read.format("delta").load(self.idescat_path)
            loaded_data['lloguer'] = self.spark.read.format("delta").load(self.lloguer_path)
            loaded_data['rfdbc'] = self.spark.read.format("delta").load(self.rfdbc_path)
            print("Delta tables loaded successfully.")

            # Load JSON files
            print("Loading geographic relation JSON files...")
            with open(self.mun_neighbors_path, 'r', encoding='utf-8') as f:
                loaded_data['mun_neighbors'] = json.load(f)
            with open(self.com_neighbors_path, 'r', encoding='utf-8') as f:
                loaded_data['com_neighbors'] = json.load(f)
            with open(self.com_to_prov_path, 'r', encoding='utf-8') as f:
                loaded_data['com_to_prov'] = json.load(f)
            print("JSON files loaded successfully.")

            return loaded_data
        except Exception as e:
            print(f"!!! ERROR during data loading: {e}")
            traceback.print_exc()
            raise

    def _process_idescat_spark(self, idescat_df: DataFrame) -> DataFrame:
        """
        Processes Idescat data to get the latest value for each indicator
        per municipality and pivots it into a wide format.
        """
        print("\n--- Step 2a: Processing Idescat data in Spark (get latest values & pivot) ---")
        
        window_spec = Window.partitionBy("municipality_id", "indicator_id") \
                            .orderBy(F.col("reference_year").desc())

        df_latest_indicators = idescat_df \
            .filter(F.col("indicator_id").isin(list(self.static_indicator_map.keys()))) \
            .withColumn("rank", F.row_number().over(window_spec)) \
            .filter(F.col("rank") == 1) \
            .drop("rank")
        
        pivoted_df = df_latest_indicators.groupBy("municipality_id", "municipality_name", "comarca_name") \
            .pivot("indicator_id", list(self.static_indicator_map.keys())) \
            .agg(F.first("municipality_value"))
            
        for indicator_code, property_name in self.static_indicator_map.items():
            if indicator_code in pivoted_df.columns:
                pivoted_df = pivoted_df.withColumnRenamed(indicator_code, property_name)
        
        print("Idescat data pivoted successfully.")
        pivoted_df.printSchema()
        return pivoted_df

    def _process_annual_data_spark(self, lloguer_df: DataFrame, rfdbc_df: DataFrame) -> DataFrame:
        """Processes and joins annual lloguer and RFDBC data in Spark."""
        print("\n--- Step 2b: Processing annual data in Spark (rent & income) ---")

        lloguer_annual = lloguer_df \
            .filter(F.col("ambit_territorial") == "Municipi") \
            .groupBy("codi_territorial", "any") \
            .agg(F.avg("renda").alias("avg_monthly_rent")) \
            .withColumnRenamed("codi_territorial", "municipality_id")
        
        rfdbc_annual = rfdbc_df \
            .filter(F.col("indicator_code") == "PER_CAPITA_EUR") \
            .select(
                F.col("municipality_id"),
                F.col("year").alias("any"),
                F.col("value").alias("household_income")
            )
        
        annual_data_df = lloguer_annual.join(
            rfdbc_annual,
            on=["municipality_id", "any"],
            how="full_outer"
        )
        
        print("Annual rent and income data joined successfully.")
        annual_data_df.printSchema()
        return annual_data_df

    def _populate_static_entities(self, idescat_pivoted_df: DataFrame, comarca_to_province_map: dict):
        """Populates the KG with static entities (Municipalities, Comarques, Provinces) and their properties."""
        print("\n--- Step 3a: Populating static entities (Municipalities, Comarques, Provinces) ---")
        
        row_iterator = idescat_pivoted_df.toLocalIterator()

        for row in tqdm(row_iterator, desc="Processing Municipalities"):
            mun_id = row['municipality_id']
            mun_name = row['municipality_name']
            com_name = row['comarca_name']
            prov_name = comarca_to_province_map.get(com_name)

            if not all([mun_id, mun_name, com_name, prov_name]):
                continue
            
            mun_uri = self.PROJ[f"municipality/{mun_id}"]
            com_uri = self.PROJ[f"comarca/{com_name.replace(' ', '_')}"]
            prov_uri = self.PROJ[f"province/{prov_name.replace(' ', '_')}"]

            self.graph.add((mun_uri, RDF.type, self.PROJ.Municipality))
            self.graph.add((mun_uri, RDFS.label, Literal(mun_name, lang="ca")))
            self.graph.add((com_uri, RDF.type, self.PROJ.Comarca))
            self.graph.add((com_uri, RDFS.label, Literal(com_name, lang="ca")))
            self.graph.add((prov_uri, RDF.type, self.PROJ.Province))
            self.graph.add((prov_uri, RDFS.label, Literal(prov_name, lang="ca")))

            self.graph.add((mun_uri, self.PROJ.isInComarca, com_uri))
            self.graph.add((com_uri, self.PROJ.isInProvince, prov_uri))

            for prop_name in self.static_indicator_map.values():
                if prop_name in row and row[prop_name] is not None:
                    value = row[prop_name]
                    prop_uri = self.PROJ[prop_name]
                    if 'population' in prop_name.lower() or 'births' in prop_name.lower() or 'unemployment' in prop_name.lower():
                        datatype = XSD.integer
                    elif 'latitude' in prop_name.lower() or 'longitude' in prop_name.lower():
                        datatype = XSD.decimal
                    else:
                        datatype = XSD.double
                    self.graph.add((mun_uri, prop_uri, Literal(value, datatype=datatype)))
    
    def _populate_geographic_relations(self, mun_neighbors: dict, com_neighbors: dict):
        """Populates the KG with neighborhood/adjacency relations."""
        print("\n--- Step 3b: Populating geographic relations ---")
        
        for mun_id, data in tqdm(mun_neighbors.items(), desc="Municipality Neighbors"):
            mun_uri = self.PROJ[f"municipality/{mun_id}"]
            for neighbor in data['neighbors']:
                neighbor_id = neighbor['id']
                neighbor_uri = self.PROJ[f"municipality/{neighbor_id}"]
                self.graph.add((mun_uri, self.PROJ.isNeighborOf, neighbor_uri))
                self.graph.add((neighbor_uri, self.PROJ.isNeighborOf, mun_uri))

        for com_name, data in tqdm(com_neighbors.items(), desc="Comarca Neighbors"):
            com_uri = self.PROJ[f"comarca/{com_name.replace(' ', '_')}"]
            for neighbor_name in data['neighbors']:
                neighbor_uri = self.PROJ[f"comarca/{neighbor_name.replace(' ', '_')}"]
                self.graph.add((com_uri, self.PROJ.isAdjacentTo, neighbor_uri))
                self.graph.add((neighbor_uri, self.PROJ.isAdjacentTo, com_uri))

    def _populate_temporal_data(self, annual_df: DataFrame):
        """Populates the KG with temporal data points (rent, income)."""
        print("\n--- Step 3c: Populating temporal data points ---")
        
        row_iterator = annual_df.toLocalIterator()

        for row in tqdm(row_iterator, desc="Processing Annual Data"):
            mun_id = row['municipality_id']
            year = row['any']
            
            if not mun_id or not year:
                continue

            datapoint_uri = self.PROJ[f"datapoint/{mun_id}_{year}"]
            mun_uri = self.PROJ[f"municipality/{mun_id}"]

            self.graph.add((datapoint_uri, RDF.type, self.PROJ.AnnualDataPoint))
            self.graph.add((mun_uri, self.PROJ.hasAnnualData, datapoint_uri))
            self.graph.add((datapoint_uri, self.PROJ.referenceYear, Literal(year, datatype=XSD.gYear)))
            
            if 'avg_monthly_rent' in row and row['avg_monthly_rent'] is not None:
                self.graph.add((datapoint_uri, self.PROJ.avgMonthlyRent, Literal(row['avg_monthly_rent'], datatype=XSD.double)))
            if 'household_income' in row and row['household_income'] is not None:
                self.graph.add((datapoint_uri, self.PROJ.householdIncome, Literal(row['household_income'], datatype=XSD.double)))

    def _serialize_graph(self):
        """Saves the final populated graph to a file."""
        print("\n--- Step 4: Serializing Knowledge Graph ---")
        try:
            self.graph.serialize(destination=str(self.output_kg_path), format='turtle', encoding='utf-8')
            print(f"Knowledge Graph successfully serialized to: {self.output_kg_path}")
            print(f"Graph contains {len(self.graph)} triples.")
        except Exception as e:
            print(f"!!! ERROR during graph serialization: {e}")
            traceback.print_exc()

    def run(self):
        """Executes the full KG creation pipeline."""
        print("===== Starting Knowledge Graph Exploitation Pipeline =====")
        idescat_pivoted_df = None
        annual_data_df = None
        try:
            # 1. Load Data
            data_sources = self._load_data()
            
            # 2. Process Data in Spark
            idescat_pivoted_df = self._process_idescat_spark(data_sources['idescat'])
            annual_data_df = self._process_annual_data_spark(data_sources['lloguer'], data_sources['rfdbc'])

            # 3. Populate RDF Graph
            idescat_pivoted_df.cache()
            annual_data_df.cache()
            
            self._populate_static_entities(idescat_pivoted_df, data_sources['com_to_prov'])
            self._populate_geographic_relations(data_sources['mun_neighbors'], data_sources['com_neighbors'])
            self._populate_temporal_data(annual_data_df)

            # 4. Serialize Graph
            self._serialize_graph()
            
            print("\n===== Knowledge Graph Exploitation Pipeline Finished Successfully =====")

        except Exception as e:
            print("\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
            print(f"!!! Pipeline Execution Failed: {e}")
            traceback.print_exc()
            print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
        finally:
            if idescat_pivoted_df:
                idescat_pivoted_df.unpersist()
            if annual_data_df:
                annual_data_df.unpersist()