# src/exploitation/exploitation.py

import json
from pathlib import Path
import traceback
from tqdm import tqdm
import pandas as pd

# --- RDFLib Imports ---
from rdflib import Graph, Literal, Namespace
from rdflib.namespace import RDF, RDFS, XSD

# --- PySpark Imports ---
from pyspark.sql import SparkSession, DataFrame
from pyspark.sql import functions as F
from pyspark.sql.window import Window

import geo_relations

# --- Configuration ---
PROJ = Namespace("http://example.com/catalonia-ontology/")
TRUSTED_DIR = Path("./data/trusted/")
RELATIONS_DIR = Path("./data/relations/")
EXPLOITATION_DIR = Path("./data/exploitation/")
EXPLOITATION_DIR.mkdir(parents=True, exist_ok=True)

class ExploitationZone:
    """
    Orchestrates the creation of two key assets in the Exploitation Zone:
    1. A consolidated, annual tabular dataset for ML tasks.
    2. A comprehensive Knowledge Graph for semantic queries and embeddings.
    """

    def __init__(self, spark: SparkSession):
        self.spark = spark
        geo_relations.main() # Ensure geo-relations are up-to-date

        self.graph = self._initialize_graph()
        self.indicator_map = self._define_indicator_map()
        print("Exploitation Zone Initialized.")

    def _initialize_graph(self) -> Graph:
        g = Graph()
        g.bind("proj", PROJ)
        g.bind("rdf", RDF)
        g.bind("rdfs", RDFS)
        g.bind("xsd", XSD)
        return g

    def _define_indicator_map(self) -> dict:
        return {
            'f171': 'population', 'f36': 'populationMen', 'f42': 'populationWomen',
            'f187': 'birthsTotal', 'f183': 'populationSpanishNat',
            'f261': 'surfaceKM2', 'f262': 'densityPopKM2',
            'f328': 'longitude', 'f329': 'latitude',
            'f308': 'unemploymentTotal', 'f191': 'totalFamilyDwellings',
            'f270': 'publicLibrariesCount', 'f293': 'sportsPavilionsCount',
            'f294': 'multiSportsCourtsCount', 'f301': 'indoorPoolsCount'
        }

    def _load_data(self) -> dict:
        print("\n--- Step 1: Loading all trusted data sources ---")
        data = {}
        try:
            data['idescat'] = self.spark.read.format("delta").load(str(TRUSTED_DIR / "idescat"))
            data['lloguer'] = self.spark.read.format("delta").load(str(TRUSTED_DIR / "lloguer"))
            data['rfdbc'] = self.spark.read.format("delta").load(str(TRUSTED_DIR / "rfdbc"))
            
            with open(RELATIONS_DIR / "municipality_neighbors.json", 'r') as f: data['mun_neighbors'] = json.load(f)
            with open(RELATIONS_DIR / "comarca_neighbors.json", 'r') as f: data['com_neighbors'] = json.load(f)
            with open(RELATIONS_DIR / "comarca_to_province.json", 'r') as f: data['com_to_prov'] = json.load(f)
            print("All trusted data and relation files loaded successfully.")
            return data
        except Exception as e:
            print(f"ERROR: Could not load one or more data sources. Details: {e}")
            raise

    def _create_consolidated_table(self, data: dict) -> DataFrame:
        """
        Processes and joins all data sources into a single, wide, annual table for ML.
        This is inspired by the previous project's exploitation zone.
        """
        print("\n--- Step 2: Creating Consolidated Annual Table for ML ---")

        # --- Process Lloguer Data ---
        df_lloguer_imputed = data['lloguer'].filter(F.col("ambit_territorial") == "Municipi").withColumn(
            "renda_imputed",
            F.when(F.col("renda").isNotNull(), F.col("renda"))
             .when(F.col("tram_preus") == "<= 350 euros/mes", 325.0).when(F.col("tram_preus") == "> 350 i <= 450 euros/mes", 400.0)
             .when(F.col("tram_preus") == "> 350 i <= 500 euros/mes", 425.0).when(F.col("tram_preus") == "> 450 i <= 600 euros/mes", 525.0)
             .when(F.col("tram_preus") == "> 500 i <= 650 euros/mes", 575.0).when(F.col("tram_preus") == "> 600 euros/mes", 800.0)
             .when(F.col("tram_preus") == "> 650 euros/mes", 900.0).otherwise(None)
        )
        df_lloguer_annual = df_lloguer_imputed.groupBy("codi_territorial", "any").agg(
            F.first("nom_territori").alias("municipality_name"),
            F.sum("habitatges").alias("total_contracts"),
            (F.sum(F.col("renda_imputed") * F.col("habitatges")) / F.sum("habitatges")).alias("avg_monthly_rent")
        ).withColumnRenamed("codi_territorial", "municipality_id")

        # --- Process RFDBC Data ---
        df_rfdbc_pivoted = data['rfdbc'].groupBy("municipality_id", "year").pivot("indicator_code").agg(F.first("value")).withColumnRenamed("year", "any")
        df_rfdbc_processed = df_rfdbc_pivoted.withColumnRenamed("PER_CAPITA_EUR", "income_per_capita") \
                                            .withColumnRenamed("PER_CAPITA_INDEX", "income_index_cat_100") \
                                            .withColumnRenamed("VALUE_EK", "income_total_thousands_eur")
        
        # --- Join Annual Data ---
        df_merged_annual = df_lloguer_annual.join(
            df_rfdbc_processed, ["municipality_id", "any"], "full_outer"
        )

        # --- Get LATEST Idescat data and join ---
        window_spec = Window.partitionBy("municipality_id", "indicator_id").orderBy(F.col("reference_year").desc())
        df_idescat_latest = data['idescat'].withColumn("rank", F.row_number().over(window_spec)).filter(F.col("rank") == 1).drop("rank")
        
        df_idescat_pivoted = df_idescat_latest.groupBy("municipality_id", "comarca_name").pivot("indicator_id").agg(F.first("municipality_value"))
        
        # Rename idescat columns to be more user-friendly
        for old_name, new_name in self.indicator_map.items():
            if old_name in df_idescat_pivoted.columns:
                df_idescat_pivoted = df_idescat_pivoted.withColumnRenamed(old_name, new_name)
        
        # Join the time-series data with the static (latest) idescat data
        df_final = df_merged_annual.join(
            F.broadcast(df_idescat_pivoted), ["municipality_id"], "left"
        )
        
        return df_final

    def _populate_graph(self, idescat_df: DataFrame, annual_df: DataFrame, relations: dict):
        # This function is now simplified as it receives the exact dataframes it needs.
        # The KG generation logic remains IDENTICAL.
        print("\n--- Step 3: Populating the Knowledge Graph ---")
        
        mun_neighbors = relations['mun_neighbors']
        com_neighbors = relations['com_neighbors']
        com_to_prov = relations['com_to_prov']
        
        created_nodes = set()
        
        # ... (The entire for loop for populating the graph remains here, unchanged) ...
        print("Generating structural nodes and relationships...")
        geo_structure_df = idescat_df.select("municipality_id", "municipality_name", "comarca_name").distinct()
        for row in tqdm(geo_structure_df.collect(), desc="Creating Geo Entities"):
            mun_id, mun_name, com_name = row['municipality_id'], row['municipality_name'], row['comarca_name']
            if not mun_id or not com_name: continue

            mun_uri = PROJ[f"municipality/{mun_id}"]
            com_uri = PROJ[f"comarca/{com_name.replace(' ', '_')}"]
            
            if mun_uri not in created_nodes:
                self.graph.add((mun_uri, RDF.type, PROJ.Municipality))
                self.graph.add((mun_uri, RDFS.label, Literal(mun_name, lang='ca')))
                self.graph.add((mun_uri, PROJ.isInComarca, com_uri))
                created_nodes.add(mun_uri)

            if com_uri not in created_nodes:
                prov_name = com_to_prov.get(com_name)
                self.graph.add((com_uri, RDF.type, PROJ.Comarca))
                self.graph.add((com_uri, RDFS.label, Literal(com_name, lang='ca')))
                if prov_name:
                    prov_uri = PROJ[f"province/{prov_name.replace(' ', '_')}"]
                    self.graph.add((com_uri, PROJ.isInProvince, prov_uri))
                    if prov_uri not in created_nodes:
                        self.graph.add((prov_uri, RDF.type, PROJ.Province))
                        self.graph.add((prov_uri, RDFS.label, Literal(prov_name, lang='ca')))
                        created_nodes.add(prov_uri)
                created_nodes.add(com_uri)

        for mun_id, data in mun_neighbors.items():
            for neighbor in data['neighbors']:
                self.graph.add((PROJ[f"municipality/{mun_id}"], PROJ.isNeighborOf, PROJ[f"municipality/{neighbor['id']}"]))
        for com_name, data in com_neighbors.items():
            for neighbor_name in data['neighbors']:
                self.graph.add((PROJ[f"comarca/{com_name.replace(' ', '_')}"], PROJ.isAdjacentTo, PROJ[f"comarca/{neighbor_name.replace(' ', '_')}"]))

        print("Generating nodes for Idescat indicator concepts...")
        for indicator_name in self.indicator_map.values():
            indicator_uri = PROJ[f"indicator/{indicator_name}"]
            if indicator_uri not in created_nodes:
                self.graph.add((indicator_uri, RDF.type, PROJ.IdescatIndicator))
                self.graph.add((indicator_uri, RDFS.label, Literal(indicator_name)))
                created_nodes.add(indicator_uri)

        print("Generating Idescat latest-value observations...")
        window_spec = Window.partitionBy("municipality_id", "indicator_id").orderBy(F.col("reference_year").desc())
        idescat_latest_df = idescat_df.withColumn("rank", F.row_number().over(window_spec)).filter(F.col("rank") == 1).drop("rank")
        
        for row in tqdm(idescat_latest_df.collect(), desc="Creating Idescat Observations"):
            mun_id, indicator_id, value, ref_year = row['municipality_id'], row['indicator_id'], row['municipality_value'], row['reference_year']
            if any(v is None for v in [mun_id, indicator_id, value, ref_year]): continue

            indicator_name = self.indicator_map.get(indicator_id)
            if not indicator_name: continue

            obs_uri = PROJ[f"observation/{mun_id}_{indicator_name}"]
            mun_uri = PROJ[f"municipality/{mun_id}"]
            indicator_uri = PROJ[f"indicator/{indicator_name}"]

            self.graph.add((obs_uri, RDF.type, PROJ.IndicatorObservation))
            self.graph.add((obs_uri, RDFS.label, Literal(f"{mun_id}_{indicator_name}")))
            self.graph.add((obs_uri, PROJ.value, Literal(value, datatype=XSD.double)))
            self.graph.add((obs_uri, PROJ.referenceYear, Literal(ref_year, datatype=XSD.gYear)))
            self.graph.add((mun_uri, PROJ.hasObservation, obs_uri))
            self.graph.add((obs_uri, PROJ.field, indicator_uri))

        print("Generating AnnualDataPoint nodes with all annual data...")
        for row in tqdm(annual_df.toLocalIterator(), desc="Creating Annual Data Points"):
            mun_id = row['municipality_id']
            year = row['any']
            
            mun_uri = PROJ[f"municipality/{mun_id}"]
            datapoint_uri = PROJ[f"datapoint/{mun_id}_{int(year)}"]
            
            self.graph.add((datapoint_uri, RDF.type, PROJ.AnnualDataPoint))
            self.graph.add((mun_uri, PROJ.hasAnnualData, datapoint_uri))
            
            self.graph.add((datapoint_uri, PROJ.referenceYear, Literal(int(year), datatype=XSD.gYear)))
            
            if row['avg_monthly_rent'] is not None and pd.notna(row['avg_monthly_rent']):
                self.graph.add((datapoint_uri, PROJ.avgMonthlyRent, Literal(row['avg_monthly_rent'], datatype=XSD.double)))
            if row['total_contracts'] is not None and pd.notna(row['total_contracts']):
                self.graph.add((datapoint_uri, PROJ.totalContracts, Literal(row['total_contracts'], datatype=XSD.integer)))
            if row['income_per_capita'] is not None and pd.notna(row['income_per_capita']):
                self.graph.add((datapoint_uri, PROJ.incomePerCapita, Literal(row['income_per_capita'], datatype=XSD.double)))
            if row['income_index_cat_100'] is not None and pd.notna(row['income_index_cat_100']):
                self.graph.add((datapoint_uri, PROJ.incomeIndex, Literal(row['income_index_cat_100'], datatype=XSD.double)))
            if row['income_total_thousands_eur'] is not None and pd.notna(row['income_total_thousands_eur']):
                self.graph.add((datapoint_uri, PROJ.incomeTotal, Literal(row['income_total_thousands_eur'], datatype=XSD.double)))
        
        print("Knowledge Graph population complete.")


    def run(self):
        """Executes the full Exploitation Zone pipeline."""
        try:
            # Step 1: Load all raw data
            all_data = self._load_data()
            
            # Step 2: Create and save the consolidated table needed for ML
            df_for_ml = self._create_consolidated_table(all_data)
            table_output_path = EXPLOITATION_DIR / "municipal_annual"
            
            print(f"\n--- Saving consolidated annual table to: {table_output_path} ---")
            df_for_ml.write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(str(table_output_path))
            print("Consolidated table for ML saved successfully.")
            
            # Step 3: Populate and save the Knowledge Graph (using the same data)
            # This ensures the KG remains IDENTICAL to the one used for training embeddings.
            relations_data = {k: v for k, v in all_data.items() if k.endswith('_neighbors') or k.endswith('_prov')}
            self._populate_graph(all_data['idescat'], df_for_ml, relations_data)
            
            kg_output_file = EXPLOITATION_DIR / "knowledge_graph.ttl"
            print(f"\n--- Saving Knowledge Graph to {kg_output_file} ---")
            self.graph.serialize(destination=str(kg_output_file), format='turtle')
            print(f"Knowledge Graph saved successfully. Contains {len(self.graph)} triples.")
            
            print("\n--- Exploitation Zone (KG & ML Table) Task Successfully Completed ---")
        except Exception as e:
            print(f"!!! ERROR during Exploitation Zone execution: {e}")
            traceback.print_exc()

if __name__ == "__main__":
    from spark_session import get_spark_session
    spark_session = None
    try:
        spark_session = get_spark_session()
        kg_processor = ExploitationZone(spark=spark_session)
        kg_processor.run()
    finally:
        if spark_session:
            print("\nStopping Spark Session.")
            spark_session.stop()