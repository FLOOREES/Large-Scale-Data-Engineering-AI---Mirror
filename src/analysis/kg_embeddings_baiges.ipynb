{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f989594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34manalysis\u001b[m\u001b[m        \u001b[34mexploitation\u001b[m\u001b[m    \u001b[34mlanding\u001b[m\u001b[m         \u001b[34mtrusted\u001b[m\u001b[m\n",
      "\u001b[34manalysis_output\u001b[m\u001b[m \u001b[34mformatted\u001b[m\u001b[m       \u001b[34mrelations\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls ../../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb93a2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rogerbaigess/Desktop/IA/3r/BDA/large-scale-data-engineering-ai/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyKEEN version: 1.11.1\n",
      "PyTorch version: 2.7.1\n"
     ]
    }
   ],
   "source": [
    "# --- Imports and Setup ---\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyKEEN imports\n",
    "from pykeen.pipeline import pipeline\n",
    "from pykeen.triples import TriplesFactory\n",
    "import pykeen.version\n",
    "\n",
    "# RDFLib for initial graph processing\n",
    "import rdflib\n",
    "from rdflib import Graph\n",
    "\n",
    "print(f\"PyKEEN version: {pykeen.version.get_version()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# --- Configuration ---\n",
    "KG_PATH = \"../../data/exploitation/knowledge_graph.ttl\"\n",
    "OUTPUT_DIR = Path(\"../../data/analysis/embeddings\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TRIPLES_PATH = OUTPUT_DIR / \"kg_triples_for_pykeen.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a1905d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full Knowledge Graph from: ../../data/exploitation/knowledge_graph.ttl\n",
      "Graph loaded successfully with 180513 triples.\n",
      "\n",
      "Filtering graph to exclude triples with literal objects...\n",
      "Filtered down to 78126 entity-relation-entity triples.\n",
      "Saving filtered triples to: ../../data/analysis/embeddings/kg_triples_for_pykeen.tsv\n",
      "Data preparation for PyKEEN complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Load and Filter Knowledge Graph for KGE Models ---\n",
    "\n",
    "print(f\"Loading full Knowledge Graph from: {KG_PATH}\")\n",
    "g = Graph()\n",
    "try:\n",
    "    g.parse(KG_PATH, format=\"turtle\")\n",
    "    print(f\"Graph loaded successfully with {len(g)} triples.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Knowledge Graph file not found at {KG_PATH}. Please run the exploitation zone script first.\")\n",
    "    raise\n",
    "\n",
    "# --- Data Preparation for PyKEEN ---\n",
    "# KGE models like TransE and DistMult work with entity-relation-entity triples.\n",
    "# We will filter the graph to keep ONLY these structural triples.\n",
    "# We are completely ignoring triples with literal objects for this training phase.\n",
    "\n",
    "print(\"\\nFiltering graph to exclude triples with literal objects...\")\n",
    "entity_triples = []\n",
    "for s, p, o in g:\n",
    "    # The condition is simple: the subject (s) and object (o) must be URIs (entities).\n",
    "    # The predicate (p) is always a URI.\n",
    "    if isinstance(s, rdflib.URIRef) and isinstance(o, rdflib.URIRef):\n",
    "        entity_triples.append((str(s), str(p), str(o)))\n",
    "\n",
    "print(f\"Filtered down to {len(entity_triples)} entity-relation-entity triples.\")\n",
    "\n",
    "# Save the filtered triples to a TSV file for PyKEEN\n",
    "print(f\"Saving filtered triples to: {TRIPLES_PATH}\")\n",
    "with open(TRIPLES_PATH, \"w\") as f:\n",
    "    # Add a header for clarity, though PyKEEN doesn't strictly need it\n",
    "    # f.write(\"head\\trelation\\ttail\\n\") \n",
    "    for s, p, o in entity_triples:\n",
    "        f.write(f\"{s}\\t{p}\\t{o}\\n\")\n",
    "\n",
    "print(\"Data preparation for PyKEEN complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9178d373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split successfully:\n",
      "  Training triples:   62500\n",
      "  Validation triples: 7813\n",
      "  Testing triples:    7813\n",
      "  Total entities:     29332\n",
      "  Total relations:    16\n"
     ]
    }
   ],
   "source": [
    "# --- Create and Split PyKEEN Dataset ---\n",
    "\n",
    "# Create a TriplesFactory from our TSV file\n",
    "tf = TriplesFactory.from_path(TRIPLES_PATH, create_inverse_triples=True) # Adding inverse triples can help some models\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "training_set, validation_set, testing_set = tf.split([0.8, 0.1, 0.1], random_state=42)\n",
    "\n",
    "print(\"Dataset split successfully:\")\n",
    "print(f\"  Training triples:   {training_set.num_triples}\")\n",
    "print(f\"  Validation triples: {validation_set.num_triples}\")\n",
    "print(f\"  Testing triples:    {testing_set.num_triples}\")\n",
    "print(f\"  Total entities:     {tf.num_entities}\")\n",
    "print(f\"  Total relations:    {tf.num_relations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c82f5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Function for Training and Evaluation ---\n",
    "\n",
    "def run_kge_pipeline(training, validation, testing, model_name, embedding_dim=100, epochs=100):\n",
    "    \"\"\"\n",
    "    Runs the PyKEEN pipeline for a given model and returns the results.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*20} Starting Pipeline for {model_name} {'='*20}\")\n",
    "    \n",
    "    result = pipeline(\n",
    "        training=training,\n",
    "        validation=validation,\n",
    "        testing=testing,\n",
    "        model=model_name,\n",
    "        model_kwargs=dict(embedding_dim=embedding_dim),\n",
    "        training_kwargs=dict(\n",
    "            num_epochs=epochs,\n",
    "            batch_size=256,\n",
    "            use_tqdm_batch=False\n",
    "        ),\n",
    "        negative_sampler='basic',\n",
    "        evaluation_kwargs=dict(batch_size=256),\n",
    "        stopper='early',\n",
    "        stopper_kwargs=dict(frequency=5, patience=3, metric='hits@10'),\n",
    "        random_seed=42,\n",
    "        device='cpu',\n",
    "    )\n",
    "    \n",
    "    print(f\"--- Pipeline for {model_name} complete. ---\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b62f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Starting Pipeline for TransE ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pykeen.triples.triples_factory:Creating inverse triples.\n",
      "/Users/rogerbaigess/Desktop/IA/3r/BDA/large-scale-data-engineering-ai/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Training epochs on cpu:   0%|          | 0/100 [00:00<?, ?epoch/s]INFO:pykeen.triples.triples_factory:Creating inverse triples.\n",
      "Training epochs on cpu:   4%|▍         | 4/100 [00:17<05:34,  3.49s/epoch, loss=0.256, prev_loss=0.317]WARNING:torch_max_mem.api:Encountered tensors on device_types={'cpu'} while only ['cuda'] are considered safe for automatic memory utilization maximization. This may lead to undocumented crashes (but can be safe, too).\n"
     ]
    }
   ],
   "source": [
    "# --- Train and Evaluate Selected Models ---\n",
    "\n",
    "results = {}\n",
    "\n",
    "# --- Train TransE ---\n",
    "transe_result = run_kge_pipeline(\n",
    "    training=training_set,\n",
    "    validation=validation_set,\n",
    "    testing=testing_set,\n",
    "    model_name='TransE',\n",
    "    embedding_dim=50, # Can start with smaller embeddings\n",
    "    epochs=100\n",
    ")\n",
    "results['TransE'] = transe_result\n",
    "transe_result.save_to_directory(OUTPUT_DIR / 'transe_model')\n",
    "\n",
    "\n",
    "# --- Train DistMult ---\n",
    "distmult_result = run_kge_pipeline(\n",
    "    training=training_set,\n",
    "    validation=validation_set,\n",
    "    testing=testing_set,\n",
    "    model_name='DistMult',\n",
    "    embedding_dim=50,\n",
    "    epochs=100\n",
    ")\n",
    "results['DistMult'] = distmult_result\n",
    "distmult_result.save_to_directory(OUTPUT_DIR / 'distmult_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9b7a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compare Model Performance ---\n",
    "\n",
    "all_metrics = []\n",
    "for model_name, result in results.items():\n",
    "    metrics = result.metric_results.to_df()\n",
    "    test_metrics = metrics[metrics['Side'] == 'both'][metrics['Type'] == 'realistic'][metrics['Dataset'] == 'testing'].copy()\n",
    "    test_metrics['Model'] = model_name\n",
    "    all_metrics.append(test_metrics)\n",
    "\n",
    "if all_metrics:\n",
    "    results_df = pd.concat(all_metrics, ignore_index=True)\n",
    "    display_cols = ['Model', 'Metric', 'Value']\n",
    "    results_df_display = results_df[display_cols]\n",
    "    pivot_df = results_df_display.pivot(index='Model', columns='Metric', values='Value')\n",
    "    \n",
    "    final_metrics = ['mean_reciprocal_rank', 'hits_at_1', 'hits_at_3', 'hits_at_5', 'hits_at_10']\n",
    "    final_metrics_existing = [m for m in final_metrics if m in pivot_df.columns]\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*20 + \" Final Model Comparison \" + \"=\"*20)\n",
    "    print(\"Metrics evaluated on the testing set:\")\n",
    "    print(pivot_df[final_metrics_existing].to_string(float_format=\"%.4f\"))\n",
    "\n",
    "    # --- Plotting the results ---\n",
    "    plot_df = pivot_df[final_metrics_existing].reset_index()\n",
    "    melted_df = plot_df.melt(id_vars='Model', var_name='Metric', value_name='Score')\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.barplot(x='Metric', y='Score', hue='Model', data=melted_df)\n",
    "    plt.title('KGE Model Performance Comparison (Link Prediction)', fontsize=16)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Evaluation Metric')\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plot_path = OUTPUT_DIR / 'model_comparison.png'\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\"\\nComparison plot saved to: {plot_path}\")\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"No model results to display.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
